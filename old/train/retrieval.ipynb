{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80abf381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: c:\\Users\\tebats\\Baste\\Projects\\AmangBot\\backend2\\Dataset\\Integrated\\Latest\n",
      "Nomic model path: ../../Models/nomic-finetuned/nomic-finetuned-final\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# Paths (adjust if needed)\n",
    "BASE_DIR = os.path.abspath(\"../../backend2\")\n",
    "DATASET_DIR = os.path.abspath(os.path.join(BASE_DIR, \"./Dataset/Integrated/Latest\"))\n",
    "NOMIC_MODEL_PATH = \"../../Models/nomic-finetuned/nomic-finetuned-final\"\n",
    "\n",
    "# Thresholds\n",
    "SIMILARITY_THRESHOLD = 1.4  # FAISS L2 distance. Lower is better.\n",
    "\n",
    "print(f\"Dataset directory: {DATASET_DIR}\")\n",
    "print(f\"Nomic model path: {NOMIC_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a7fcfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 402 chunks from c:\\Users\\tebats\\Baste\\Projects\\AmangBot\\backend2\\Dataset\\Integrated\\Latest.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Chunks from Dataset ---\n",
    "chunks = []\n",
    "chunks_for_embedding = []\n",
    "\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    raise FileNotFoundError(f\"Dataset directory not found at {DATASET_DIR}\")\n",
    "\n",
    "for filename in os.listdir(DATASET_DIR):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(DATASET_DIR, filename)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                file_data = json.load(f)\n",
    "                if isinstance(file_data, list):\n",
    "                    for item in file_data:\n",
    "                        if \"content\" in item:\n",
    "                            chunks.append(item[\"content\"])\n",
    "                            \n",
    "                            # Prepare embedding with metadata (excluding id and keywords)\n",
    "                            metadata_parts = []\n",
    "                            for k, v in item.items():\n",
    "                                if k not in [\"id\", \"keywords\", \"content\"]:\n",
    "                                    metadata_parts.append(f\"{k}: {v}\")\n",
    "                            metadata_str = \", \".join(metadata_parts)\n",
    "                            \n",
    "                            chunks_for_embedding.append(f\"search_document: {metadata_str}. {item['content']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks from {DATASET_DIR}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54a60370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are trying to use a model that was created with Sentence Transformers version 5.1.2, but you're currently using version 5.1.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Nomic Embedder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedder initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize Embedder ---\n",
    "print(\"Initializing Nomic Embedder...\")\n",
    "embedder = SentenceTransformer(NOMIC_MODEL_PATH, trust_remote_code=True)\n",
    "print(\"Embedder initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d287ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS index...\n",
      "FAISS index built with 402 vectors.\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# --- Build FAISS Index ---\n",
    "print(\"Building FAISS index...\")\n",
    "embeddings = embedder.encode(chunks_for_embedding, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
    "print(f\"Embedding dimension: {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7cec102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize Reranker ---\n",
    "try:\n",
    "    reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    print(\"Reranker initialized.\")\n",
    "except Exception as e:\n",
    "    reranker = None\n",
    "    print(f\"Reranker failed to initialize: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e21761ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval function defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Retrieval Function ---\n",
    "def retrieve(query: str, k: int = 4):\n",
    "    \"\"\"\n",
    "    Retrieve relevant chunks for a given query.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (final_chunks, distances, is_relevant)\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_text = f\"search_query: {query}\"\n",
    "    query_embedding = embedder.encode([query_text], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # Fetch candidates (3x for reranking)\n",
    "    fetch_k = k * 3 if reranker else k\n",
    "    D, I = index.search(query_embedding, fetch_k)\n",
    "    \n",
    "    # Check if results are relevant based on distance threshold\n",
    "    is_relevant = D[0][0] <= SIMILARITY_THRESHOLD\n",
    "    \n",
    "    retrieved_chunks = [chunks[i] for i in I[0]]\n",
    "    distances = D[0].tolist()\n",
    "    \n",
    "    # Rerank if available\n",
    "    if reranker:\n",
    "        pairs = [[query, doc] for doc in retrieved_chunks]\n",
    "        scores = reranker.predict(pairs)\n",
    "        scored_chunks = sorted(zip(retrieved_chunks, scores, distances), key=lambda x: x[1], reverse=True)\n",
    "        final_chunks = [chunk for chunk, score, dist in scored_chunks[:k]]\n",
    "        final_scores = [score for chunk, score, dist in scored_chunks[:k]]\n",
    "        return final_chunks, final_scores, is_relevant\n",
    "    else:\n",
    "        return retrieved_chunks[:k], distances[:k], is_relevant\n",
    "\n",
    "print(\"Retrieval function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Retrieval ---\n",
    "# Example queries to test\n",
    "test_queries = [\n",
    "    \"What are the admission requirements?\",\n",
    "    \"Who is the president of EARIST?\",\n",
    "    \"What are the school fees?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    results, scores, is_relevant = retrieve(query, k=3)\n",
    "    \n",
    "    print(f\"Is relevant (within threshold): {is_relevant}\")\n",
    "    print(f\"\\nTop {len(results)} results:\")\n",
    "    \n",
    "    for i, (chunk, score) in enumerate(zip(results, scores), 1):\n",
    "        print(f\"\\n--- Result {i} (Score: {score:.4f}) ---\")\n",
    "        print(chunk[:500] + \"...\" if len(chunk) > 500 else chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a6d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interactive Testing ---\n",
    "# Run your own queries here\n",
    "custom_query = \"What are the requirements for enrollment?\"\n",
    "\n",
    "results, scores, is_relevant = retrieve(custom_query, k=4)\n",
    "\n",
    "print(f\"Query: {custom_query}\")\n",
    "print(f\"Is relevant: {is_relevant}\")\n",
    "print(f\"\\nRetrieved {len(results)} chunks:\\n\")\n",
    "\n",
    "for i, (chunk, score) in enumerate(zip(results, scores), 1):\n",
    "    print(f\"--- Chunk {i} (Score: {score:.4f}) ---\")\n",
    "    print(chunk)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
