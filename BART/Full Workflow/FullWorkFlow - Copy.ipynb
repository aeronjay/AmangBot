{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daad5b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "Transformers version: 4.57.1\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "GPU Memory: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f610c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 171,284 characters\n",
      "Estimated pages: ~85\n"
     ]
    }
   ],
   "source": [
    "with open('../Dataset/StudentHandbookDataset.txt', 'r', encoding='utf-8') as f:\n",
    "    dataset = f.read()\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset):,} characters\")\n",
    "print(f\"Estimated pages: ~{len(dataset) // 2000}\")\n",
    "\n",
    "# Import retrieval dependencies\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ca3a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Configuration:\n",
      "\n",
      "CHUNKING:\n",
      "  strategy: semantic\n",
      "  percentile_threshold: 50\n",
      "  buffer_size: 1\n",
      "\n",
      "EMBEDDING:\n",
      "  model_name: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
      "  device: cuda\n",
      "\n",
      "GENERATION:\n",
      "  model_name: mistralai/Mistral-7B-Instruct-v0.1\n",
      "  max_length: 2048\n",
      "  temperature: 0.3\n",
      "  top_p: 0.9\n",
      "  do_sample: True\n",
      "\n",
      "RETRIEVAL:\n",
      "  top_k: 5\n",
      "  score_threshold: 0.3\n"
     ]
    }
   ],
   "source": [
    "# RAG Configuration - Using your optimal settings from retrieval experiments\n",
    "RAG_CONFIG = {\n",
    "    'chunking': {\n",
    "        'strategy': 'semantic',\n",
    "        'percentile_threshold': 50,\n",
    "        'buffer_size': 1\n",
    "    },\n",
    "    'embedding': {\n",
    "        'model_name': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    },\n",
    "    'generation': {\n",
    "        'model_name': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "        'max_length': 2048,\n",
    "        'temperature': 0.3,\n",
    "        'top_p': 0.9,\n",
    "        'do_sample': True\n",
    "    },\n",
    "    'retrieval': {\n",
    "        'top_k': 5,\n",
    "        'score_threshold': 0.3\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"RAG Configuration:\")\n",
    "for section, config in RAG_CONFIG.items():\n",
    "    print(f\"\\n{section.upper()}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3111c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading custom chunks from ../Dataset/Chunks.json\n",
      "Loaded 125 custom chunks\n",
      "\n",
      "Chunk Analysis:\n",
      "  Total chunks: 125\n",
      "  Average size: 1336 characters\n",
      "  Size range: 213 - 3785 characters\n"
     ]
    }
   ],
   "source": [
    "# def load_or_create_chunks():\n",
    "#     \"\"\"Load existing chunks or create new ones with optimal config\"\"\"\n",
    "#     chunk_filename = f\"../Retriever/saved_chunks/chunks_p{RAG_CONFIG['chunking']['percentile_threshold']}_b{RAG_CONFIG['chunking']['buffer_size']}.pkl\"\n",
    "    \n",
    "#     if os.path.exists(chunk_filename):\n",
    "#         print(f\"Loading existing chunks from {chunk_filename}\")\n",
    "#         with open(chunk_filename, 'rb') as f:\n",
    "#             chunks = pickle.load(f)\n",
    "#         print(f\"Loaded {len(chunks)} chunks\")\n",
    "#         return chunks\n",
    "    \n",
    "#     print(\"Creating new chunks with optimal configuration...\")\n",
    "#     embedding_model = HuggingFaceEmbeddings(\n",
    "#         model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # Lightweight for chunking\n",
    "#         model_kwargs={'device': RAG_CONFIG['embedding']['device']},\n",
    "#         encode_kwargs={'normalize_embeddings': True}\n",
    "#     )\n",
    "    \n",
    "#     text_splitter = SemanticChunker(\n",
    "#         embeddings=embedding_model,\n",
    "#         breakpoint_threshold_type=\"percentile\",\n",
    "#         breakpoint_threshold_amount=RAG_CONFIG['chunking']['percentile_threshold'],\n",
    "#         buffer_size=RAG_CONFIG['chunking']['buffer_size'],\n",
    "#         add_start_index=True\n",
    "#     )\n",
    "    \n",
    "#     chunks = text_splitter.create_documents([dataset])\n",
    "    \n",
    "#     # Save chunks\n",
    "#     os.makedirs(\"../Retriever/saved_chunks\", exist_ok=True)\n",
    "#     with open(chunk_filename, 'wb') as f:\n",
    "#         pickle.dump(chunks, f)\n",
    "    \n",
    "#     print(f\"Created and saved {len(chunks)} chunks\")\n",
    "#     return chunks\n",
    "\n",
    "# chunks = load_or_create_chunks()\n",
    "\n",
    "# # Analyze chunks\n",
    "# chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "# print(f\"\\nChunk Analysis:\")\n",
    "# print(f\"  Total chunks: {len(chunks)}\")\n",
    "# print(f\"  Average size: {np.mean(chunk_sizes):.0f} characters\")\n",
    "# print(f\"  Size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters\")\n",
    "\n",
    "import json\n",
    "\n",
    "def load_or_create_chunks():\n",
    "    \"\"\"Load custom chunks from Chunks.json\"\"\"\n",
    "    chunks_file = \"../Dataset/Chunks.json\"\n",
    "    \n",
    "    print(f\"Loading custom chunks from {chunks_file}\")\n",
    "    \n",
    "    try:\n",
    "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "            chunks_data = json.load(f)\n",
    "        \n",
    "        # Convert JSON chunks to LangChain Document format\n",
    "        from langchain.schema import Document\n",
    "        \n",
    "        chunks = []\n",
    "        for chunk in chunks_data:\n",
    "            doc = Document(\n",
    "                page_content=chunk['content'],\n",
    "                metadata={\n",
    "                    'chunk_id': chunk['chunk_id'],\n",
    "                    'source_document': chunk['metadata']['source_document'],\n",
    "                    'section_hierarchy': chunk['metadata']['section_hierarchy']\n",
    "                }\n",
    "            )\n",
    "            chunks.append(doc)\n",
    "        \n",
    "        print(f\"Loaded {len(chunks)} custom chunks\")\n",
    "        return chunks\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {chunks_file} not found!\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading chunks: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load chunks\n",
    "chunks = load_or_create_chunks()\n",
    "\n",
    "if chunks:\n",
    "    # Analyze chunks\n",
    "    chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "    print(f\"\\nChunk Analysis:\")\n",
    "    print(f\"  Total chunks: {len(chunks)}\")\n",
    "    print(f\"  Average size: {np.mean(chunk_sizes):.0f} characters\")\n",
    "    print(f\"  Size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters\")\n",
    "else:\n",
    "    print(\"Failed to load chunks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e14d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retrieval system...\n",
      "Setting up embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebats\\AppData\\Local\\Temp\\ipykernel_10212\\2777203703.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embedding_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for all chunks...\n",
      "Building FAISS index...\n",
      "Retrieval system ready: 125 vectors (768D)\n",
      "Retrieval system ready!\n"
     ]
    }
   ],
   "source": [
    "class OptimalRetriever:\n",
    "    \"\"\"Retrieval system using your best-performing configuration\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks, config):\n",
    "        self.chunks = chunks\n",
    "        self.config = config\n",
    "        self.embedding_model = None\n",
    "        self.index = None\n",
    "        self.setup_embeddings()\n",
    "    \n",
    "    def setup_embeddings(self):\n",
    "        \"\"\"Initialize embedding model and FAISS index\"\"\"\n",
    "        print(\"Setting up embedding model...\")\n",
    "        self.embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=self.config['embedding']['model_name'],\n",
    "            model_kwargs={\n",
    "                'device': self.config['embedding']['device'],\n",
    "                'trust_remote_code': True   \n",
    "            },\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        print(\"Generating embeddings for all chunks...\")\n",
    "        chunk_texts = [chunk.page_content for chunk in self.chunks]\n",
    "        \n",
    "        # Batch processing to avoid memory issues\n",
    "        batch_size = 32\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(chunk_texts), batch_size):\n",
    "            batch = chunk_texts[i:i+batch_size]\n",
    "            batch_embeddings = self.embedding_model.embed_documents(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i + len(batch)}/{len(chunk_texts)} chunks\")\n",
    "        \n",
    "        # Build FAISS index\n",
    "        print(\"Building FAISS index...\")\n",
    "        dimension = len(all_embeddings[0])\n",
    "        self.index = faiss.IndexFlatIP(dimension)\n",
    "        \n",
    "        embeddings_array = np.array(all_embeddings).astype('float32')\n",
    "        faiss.normalize_L2(embeddings_array)\n",
    "        self.index.add(embeddings_array)\n",
    "        \n",
    "        print(f\"Retrieval system ready: {self.index.ntotal:,} vectors ({dimension}D)\")\n",
    "    \n",
    "    def retrieve(self, query, top_k=None, score_threshold=None):\n",
    "        \"\"\"Retrieve relevant chunks for a query\"\"\"\n",
    "        top_k = top_k or self.config['retrieval']['top_k']\n",
    "        score_threshold = score_threshold or self.config['retrieval']['score_threshold']\n",
    "        \n",
    "        # Embed query\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        query_vector = np.array([query_embedding]).astype('float32')\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_vector, top_k)\n",
    "        \n",
    "        # Filter by threshold and format results\n",
    "        relevant_chunks = []\n",
    "        for idx, score in zip(indices[0], scores[0]):\n",
    "            if score >= score_threshold and idx < len(self.chunks):\n",
    "                relevant_chunks.append({\n",
    "                    'text': self.chunks[idx].page_content,\n",
    "                    'score': float(score),\n",
    "                    'chunk_id': int(idx)\n",
    "                })\n",
    "        \n",
    "        return relevant_chunks\n",
    "\n",
    "# Initialize retriever\n",
    "print(\"Initializing retrieval system...\")\n",
    "retriever = OptimalRetriever(chunks, RAG_CONFIG)\n",
    "print(\"Retrieval system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8043d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Logging in to Hugging Face...\n",
      "You'll be prompted to enter your HF token.\n",
      "Get your token from: https://huggingface.co/settings/tokens\n",
      "\n",
      "Note: The token input will be hidden for security.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bcd06944c74d8b952d777d00026089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face to download Mistral-7B\n",
    "print(\"üîê Logging in to Hugging Face...\")\n",
    "print(\"You'll be prompted to enter your HF token.\")\n",
    "print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
    "print(\"\\nNote: The token input will be hidden for security.\")\n",
    "\n",
    "try:\n",
    "    login()\n",
    "    print(\"‚úÖ Successfully logged in to Hugging Face!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Login failed: {e}\")\n",
    "    print(\"\\nAlternative: You can set your token in environment variable:\")\n",
    "    print(\"HF_TOKEN=your_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17bddb",
   "metadata": {},
   "source": [
    "## üîê Step 6: Hugging Face Authentication\n",
    "\n",
    "Before downloading Mistral-7B, you need to authenticate with Hugging Face. This is required to access gated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d83583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Still working... please wait...üöÄ LOADING MISTRAL-7B FOR RTX 5050 (8GB VRAM)\n",
      "\n",
      "======================================================================\n",
      "GPU: NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "Total VRAM: 8.5 GB\n",
      "Initial GPU memory: 0.45 GB\n",
      "\n",
      "üìù Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\tokenizer.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded successfully\n",
      "\n",
      "‚öôÔ∏è Configuring 4-bit quantization for 8GB VRAM...\n",
      "‚úÖ Quantization config ready (4-bit NF4 + double quant)\n",
      "\n",
      "üîÑ Loading mistralai/Mistral-7B-Instruct-v0.1 with 4-bit quantization...\n",
      "üì¶ This will download ~13GB if not cached (may take 5-10 minutes)\n",
      "‚è≥ Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\config.json\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\model.safetensors.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1e3d017789464e9ae708c69ce6764a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside mistralai/Mistral-7B-Instruct-v0.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ MODEL LOADED SUCCESSFULLY!\n",
      "======================================================================\n",
      "üíæ GPU Memory Status:\n",
      "   Allocated:  4.58 GB\n",
      "   Reserved:   4.70 GB\n",
      "   Free:       3.30 GB\n",
      "   Total:      8.00 GB\n",
      "   Usage:      58.8%\n",
      "   Status:     üü¢ Excellent - Plenty of headroom\n",
      "\n",
      "üéØ Optimizations Applied for 8GB VRAM:\n",
      "   ‚úÖ 4-bit NF4 quantization (~4GB model size)\n",
      "   ‚úÖ Double quantization for extra savings\n",
      "   ‚úÖ Float16 compute dtype\n",
      "   ‚úÖ Automatic device mapping\n",
      "   ‚úÖ Low CPU memory usage mode\n",
      "\n",
      "üìä Model Configuration:\n",
      "   Model:       mistralai/Mistral-7B-Instruct-v0.1\n",
      "   Precision:   4-bit (quantized from 16-bit)\n",
      "   Device:      cuda:0\n",
      "   Parameters:  ~7B (quantized)\n",
      "\n",
      "======================================================================\n",
      "üèÅ Final Status: ‚úÖ READY TO USE\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import asyncio\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "# ==========================================================\n",
    "# Optimized Model Loading for RTX 5050 (8GB VRAM) ‚Äî with Live Output\n",
    "# ==========================================================\n",
    "import sys, os, time, threading\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.utils import logging\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# === Enable live print output and progress ===\n",
    "os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "logging.set_verbosity_info()\n",
    "logging.enable_progress_bar()\n",
    "\n",
    "def heartbeat():\n",
    "    while True:\n",
    "        print(\"‚è≥ Still working... please wait...\", flush=True)\n",
    "        time.sleep(60)\n",
    "\n",
    "threading.Thread(target=heartbeat, daemon=True).start()\n",
    "\n",
    "# ==========================================================\n",
    "print(\"üöÄ LOADING MISTRAL-7B FOR RTX 5050 (8GB VRAM)\", flush=True)\n",
    "print(\"=\" * 70, flush=True)\n",
    "\n",
    "model_name = RAG_CONFIG['generation']['model_name']\n",
    "model_loaded = False\n",
    "mistral_model = None\n",
    "mistral_tokenizer = None\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\", flush=True)\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\", flush=True)\n",
    "    print(f\"Initial GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\", flush=True)\n",
    "\n",
    "try:\n",
    "    # Step 1: Load Tokenizer\n",
    "    print(f\"\\nüìù Loading tokenizer for {model_name}...\", flush=True)\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if mistral_tokenizer.pad_token is None:\n",
    "        mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "    print(\"‚úÖ Tokenizer loaded successfully\", flush=True)\n",
    "    \n",
    "    # Step 2: Configure 4-bit Quantization (Essential for 8GB VRAM)\n",
    "    print(\"\\n‚öôÔ∏è Configuring 4-bit quantization for 8GB VRAM...\", flush=True)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                      # Use 4-bit quantization\n",
    "        bnb_4bit_compute_dtype=torch.float16,   # Compute in float16\n",
    "        bnb_4bit_use_double_quant=True,         # Double quantization for extra memory savings\n",
    "        bnb_4bit_quant_type=\"nf4\"               # NormalFloat 4-bit quantization\n",
    "    )\n",
    "    print(\"‚úÖ Quantization config ready (4-bit NF4 + double quant)\", flush=True)\n",
    "    \n",
    "    # Step 3: Load Model with Quantization\n",
    "    print(f\"\\nüîÑ Loading {model_name} with 4-bit quantization...\", flush=True)\n",
    "    print(\"üì¶ This will download ~13GB if not cached (may take 5-10 minutes)\", flush=True)\n",
    "    print(\"‚è≥ Please wait...\", flush=True)\n",
    "    \n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",                      # Automatically distribute across GPU\n",
    "        torch_dtype=torch.float16,              # Use float16 for memory efficiency\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True                  # Minimize CPU memory during loading\n",
    "    )\n",
    "    \n",
    "    # Clear cache after loading\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    model_loaded = True\n",
    "    \n",
    "    # Display Success and Memory Stats\n",
    "    print(\"\\n\" + \"=\" * 70, flush=True)\n",
    "    print(\"‚úÖ MODEL LOADED SUCCESSFULLY!\", flush=True)\n",
    "    print(\"=\" * 70, flush=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        memory_free = 8 - memory_reserved\n",
    "        \n",
    "        print(f\"üíæ GPU Memory Status:\", flush=True)\n",
    "        print(f\"   Allocated:  {memory_allocated:.2f} GB\", flush=True)\n",
    "        print(f\"   Reserved:   {memory_reserved:.2f} GB\", flush=True)\n",
    "        print(f\"   Free:       {memory_free:.2f} GB\", flush=True)\n",
    "        print(f\"   Total:      8.00 GB\", flush=True)\n",
    "        print(f\"   Usage:      {(memory_reserved/8)*100:.1f}%\", flush=True)\n",
    "        \n",
    "        # Memory status indicator\n",
    "        if memory_reserved < 6.4:  # < 80%\n",
    "            print(f\"   Status:     üü¢ Excellent - Plenty of headroom\", flush=True)\n",
    "        elif memory_reserved < 7.2:  # < 90%\n",
    "            print(f\"   Status:     üü° Good - Monitor memory usage\", flush=True)\n",
    "        else:\n",
    "            print(f\"   Status:     üî¥ Tight - Be cautious with batch sizes\", flush=True)\n",
    "    \n",
    "    print(\"\\nüéØ Optimizations Applied for 8GB VRAM:\", flush=True)\n",
    "    print(\"   ‚úÖ 4-bit NF4 quantization (~4GB model size)\", flush=True)\n",
    "    print(\"   ‚úÖ Double quantization for extra savings\", flush=True)\n",
    "    print(\"   ‚úÖ Float16 compute dtype\", flush=True)\n",
    "    print(\"   ‚úÖ Automatic device mapping\", flush=True)\n",
    "    print(\"   ‚úÖ Low CPU memory usage mode\", flush=True)\n",
    "    \n",
    "    print(\"\\nüìä Model Configuration:\", flush=True)\n",
    "    print(f\"   Model:       {model_name}\", flush=True)\n",
    "    print(f\"   Precision:   4-bit (quantized from 16-bit)\", flush=True)\n",
    "    print(f\"   Device:      {next(mistral_model.parameters()).device}\", flush=True)\n",
    "    print(f\"   Parameters:  ~7B (quantized)\", flush=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\" * 70, flush=True)\n",
    "    print(\"‚ùå MODEL LOADING FAILED\", flush=True)\n",
    "    print(\"=\" * 70, flush=True)\n",
    "    print(f\"Error: {str(e)}\", flush=True)\n",
    "    print(\"\\nüí° Troubleshooting Steps:\", flush=True)\n",
    "    print(\"1. ‚úÖ Check that you're logged in to Hugging Face (run previous cell)\", flush=True)\n",
    "    print(\"2. ‚úÖ Ensure you have internet connection for download\", flush=True)\n",
    "    print(\"3. ‚úÖ Verify you have ~15GB free disk space\", flush=True)\n",
    "    print(\"4. ‚úÖ Check that CUDA is available: torch.cuda.is_available()\", flush=True)\n",
    "    print(\"5. ‚úÖ Try restarting the notebook kernel\", flush=True)\n",
    "    print(\"\\nüí° If issues persist:\", flush=True)\n",
    "    print(\"   - Install/update: pip install -U transformers accelerate bitsandbytes\", flush=True)\n",
    "    print(\"   - Clear HF cache: rm -rf ~/.cache/huggingface/\", flush=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    model_loaded = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70, flush=True)\n",
    "print(f\"üèÅ Final Status: {'‚úÖ READY TO USE' if model_loaded else '‚ùå NOT LOADED'}\", flush=True)\n",
    "print(\"=\" * 70, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef23ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralRAGGenerator:\n",
    "    \"\"\"Memory-optimized RAG system using Mistral-7B-Instruct for RTX 5050 (8GB VRAM)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, retriever, config):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.retriever = retriever\n",
    "        self.config = config\n",
    "\n",
    "        # üîß Optimized system prompt (uses second version's [INST] style)\n",
    "        self.system_prompt = \"\"\"<s>[INST] You are a university student advisor with access to the official student handbook.\n",
    "Your task is to answer the student's question accurately using only the provided handbook context.\n",
    "\n",
    "Guidelines:\n",
    "- Use only the provided context to answer.\n",
    "- If the context doesn‚Äôt include enough information, say so clearly.\n",
    "- Be specific about policies, procedures, and requirements.\n",
    "- Keep your response concise and factual.\n",
    "\n",
    "HANDBOOK CONTEXT:\n",
    "{context}\n",
    "\n",
    "STUDENT QUESTION:\n",
    "{question}\n",
    "\n",
    "Provide a clear, helpful answer based on the handbook context above. \n",
    "If the handbook lacks enough information, explicitly say so. [/INST]\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    def format_context(self, retrieved_chunks):\n",
    "        \"\"\"Format retrieved chunks into context string\"\"\"\n",
    "        if not retrieved_chunks:\n",
    "            return \"No relevant information found.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "            context_parts.append(f\"[Section {i}] {chunk['text']}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_response(self, question, max_new_tokens=200, temperature=None, top_p=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Generate RAG response optimized for 8GB VRAM GPUs.\n",
    "        - Uses reduced context and token limits\n",
    "        - Clears GPU memory between steps\n",
    "        - Enables KV cache for faster generation\n",
    "        \"\"\"\n",
    "        temperature = temperature or self.config['generation']['temperature']\n",
    "        top_p = top_p or self.config['generation']['top_p']\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n‚ùì Question: {question}\")\n",
    "            print(\"=\" * 70)\n",
    "        \n",
    "        # üßπ Clear GPU cache before generation\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # üîç Step 1: Retrieve relevant context\n",
    "        if verbose:\n",
    "            print(\"üîç Retrieving relevant context...\")\n",
    "        retrieved_chunks = self.retriever.retrieve(question)\n",
    "        \n",
    "        if not retrieved_chunks:\n",
    "            if verbose:\n",
    "                print(\"‚ö†Ô∏è No relevant context found.\")\n",
    "            return {\n",
    "                'question': question,\n",
    "                'response': \"I couldn‚Äôt find relevant information in the student handbook to answer your question.\",\n",
    "                'retrieved_chunks': [],\n",
    "                'context_used': \"\"\n",
    "            }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üìö Found {len(retrieved_chunks)} relevant chunks:\")\n",
    "            for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "                print(f\"   {i}. Relevance score: {chunk['score']:.4f}\")\n",
    "        \n",
    "        # üß† Step 2: Format context (limit length for VRAM efficiency)\n",
    "        context = self.format_context(retrieved_chunks)\n",
    "        context_limited = context[:1200]  # 1.2k chars keeps VRAM below 8GB\n",
    "        \n",
    "        # üß© Step 3: Construct optimized prompt\n",
    "        prompt = self.system_prompt.format(context=context_limited, question=question)\n",
    "        \n",
    "        # ‚úÇÔ∏è Step 4: Tokenize input efficiently\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024  # Lower to prevent OOM on 8GB GPUs\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # üìä GPU memory check before generation\n",
    "        if verbose and torch.cuda.is_available():\n",
    "            mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"üíæ GPU memory before generation: {mem_before:.2f} GB\")\n",
    "        \n",
    "        # ‚öôÔ∏è Step 5: Generate response (no grad, cache on)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,      # keep small for VRAM\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=self.config['generation']['do_sample'],\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                use_cache=True                       # enables faster generation\n",
    "            )\n",
    "        \n",
    "        # üßæ Step 6: Decode and extract answer\n",
    "        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        if \"Answer:\" in full_output:\n",
    "            response = full_output.split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            response = full_output[len(prompt):].strip()\n",
    "        \n",
    "        # üßπ Step 7: Cleanup GPU memory\n",
    "        del inputs, outputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            if verbose:\n",
    "                mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "                print(f\"üíæ GPU memory after cleanup: {mem_after:.2f} GB\")\n",
    "        \n",
    "        # ü™Ñ Step 8: Display and return\n",
    "        if verbose:\n",
    "            print(\"\\nüí° Answer:\")\n",
    "            print(\"-\" * 70)\n",
    "            print(response)\n",
    "            print(\"-\" * 70)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'response': response,\n",
    "            'retrieved_chunks': retrieved_chunks,\n",
    "            'context_used': context_limited\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1765af69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîß INITIALIZING RAG SYSTEM\n",
      "======================================================================\n",
      "‚úÖ RAG system initialized successfully!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG system\n",
    "if model_loaded and mistral_model is not None and mistral_tokenizer is not None:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîß INITIALIZING RAG SYSTEM\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    rag_system = MistralRAGGenerator(\n",
    "        model=mistral_model,\n",
    "        tokenizer=mistral_tokenizer,\n",
    "        retriever=retriever,\n",
    "        config=RAG_CONFIG\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ RAG system initialized successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize RAG system - model or tokenizer not loaded\")\n",
    "    rag_system = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97b4ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG system ready for testing!\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system with sample questions\n",
    "test_questions = [\n",
    "    \"What are the admission requirements?\",\n",
    "    \"What is the grading system?\",\n",
    "    \"How do I apply for financial aid?\",\n",
    "    \"What are the library hours?\",\n",
    "    \"What is the academic calendar?\"\n",
    "]\n",
    "\n",
    "def test_rag_system(questions=None):\n",
    "    \"\"\"Test RAG system with multiple questions\"\"\"\n",
    "    if not model_loaded:\n",
    "        print(\"Cannot test - model not loaded\")\n",
    "        return\n",
    "    \n",
    "    questions = questions or test_questions\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Testing RAG system with {len(questions)} questions...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\nTEST {i}/{len(questions)}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = rag_system.generate_response(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        result['elapsed_time'] = elapsed_time\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\nTime taken: {elapsed_time:.2f} seconds\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run quick test (uncomment to test)\n",
    "# test_results = test_rag_system(test_questions[:2])  # Test first 2 questions\n",
    "\n",
    "print(\"RAG system ready for testing!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0344069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Enhanced interactive chat function ready!\n",
      "======================================================================\n",
      "ü§ñ INTERACTIVE RAG CHAT - RTX 5050 OPTIMIZED\n",
      "======================================================================\n",
      "Ask questions about the student handbook!\n",
      "\n",
      "üìã Commands:\n",
      "  'quit' or 'exit' - End chat\n",
      "  'help' - Show commands\n",
      "  'memory' - Show GPU memory usage\n",
      "  'history' - Show chat history\n",
      "  'clear' - Clear chat history\n",
      "  'save' - Save chat history to file\n",
      "======================================================================\n",
      "\n",
      "üü¢ GPU Memory: 4.6GB (57%)\n",
      "\n",
      "‚ùì Question: Who is the president of EARIST?\n",
      "======================================================================\n",
      "üîç Retrieving relevant context...\n",
      "üìö Found 5 relevant chunks:\n",
      "   1. Relevance score: 0.5430\n",
      "   2. Relevance score: 0.5364\n",
      "   3. Relevance score: 0.5019\n",
      "   4. Relevance score: 0.4890\n",
      "   5. Relevance score: 0.4865\n",
      "üíæ GPU memory before generation: 4.58 GB\n",
      "üíæ GPU memory after cleanup: 4.58 GB\n",
      "\n",
      "üí° Answer:\n",
      "----------------------------------------------------------------------\n",
      "The handbook context does not provide information about the current president of EARIST.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚è±Ô∏è Response time: 33.9s\n",
      "\n",
      "üü¢ GPU Memory: 4.6GB (57%)\n",
      "\n",
      "‚ùì Question: What is CAS College\n",
      "======================================================================\n",
      "üîç Retrieving relevant context...\n",
      "üìö Found 5 relevant chunks:\n",
      "   1. Relevance score: 0.4807\n",
      "   2. Relevance score: 0.4001\n",
      "   3. Relevance score: 0.3950\n",
      "   4. Relevance score: 0.3770\n",
      "   5. Relevance score: 0.3682\n",
      "üíæ GPU memory before generation: 4.58 GB\n",
      "üíæ GPU memory after cleanup: 4.58 GB\n",
      "\n",
      "üí° Answer:\n",
      "----------------------------------------------------------------------\n",
      "The College of Arts and Sciences (CAS) is a college within the General Alvarez School of Arts and Trades (EARIST) located in EARIST Cavite Campus, General Mariano Alvarez, Cavite. The college offers several undergraduate degree programs, including Bachelor of Science in Applied Physics with Computer Science Emphasis (BSAP), Bachelor of Science in Computer Science (BSCS), Bachelor of Science in Information Technology (BS INFO. TECH.), Bachelor of Science in Psychology (BSPSYCH), and Bachelor of Science in Mathematics (BSMATH). These programs are awarded Level II status by the Accrediting Agency of Chartered Colleges and Universities in the Philippines (SUC Level II) and are also rated SUC Level II (CHED-DBM-PASUC Leveling Evaluation).\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚è±Ô∏è Response time: 80.8s\n",
      "\n",
      "üü¢ GPU Memory: 4.6GB (57%)\n",
      "üëã Chat ended. Goodbye!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'Who is the president of EARIST?',\n",
       "  'response': 'The handbook context does not provide information about the current president of EARIST.',\n",
       "  'retrieved_chunks': [{'text': 'HISTORY OF EARIST\\n\\nThe Eulogio \"Amang\" Rodriguez Institute of Science and Technology (EARIST) was established after the liberation of Manila in 1945. EARIST traces back its development from Vocational High School with only a room at the second floor of the Mapa High School, nine teachers, a clerk, and 147 students under Mr. Pantaleon Regala, its principal. Its former name was Eulogio Rodriguez Vocational High School (ERVHS).\\n\\nOn July 1, 1946, EARIST acquired its present site at Nagtahan, Sampaloc, Manila. Mr. Apolinario Apilado was appointed as principal. He was succeeded by Dr. Hilario G. Nudas in 1949.\\n\\nEARIST\\'s growth and development were made possible via three Republic Acts and Presidential Decree, to wit:\\n\\n- **Republic Act No. 4072**, jointly sponsored by Congressman Ramon D. Bagatsing and Salih Ujutalam in 1964, authorized the establishment of the Technical Education Department without changing the name of the school. It was headed by a Vocational Director.\\n- **Republic Act No. 5088**, sponsored by the late Congressman Sergio Loyola in 1967, authorized the renaming of ERVHS to Eulogio \"Amang\" Rodriguez Memorial School of Arts and Trades (EARMSAT) and signaled its separation from the Division of City Schools, Manila. It was headed by a Superintendent.\\n- **Republic Act No. 6595**, sponsored by Congressman Joaquin R. Roces in 1972, converted EARMSAT to Eulogio \"Amang\" Rodriguez Institute of Science and Technology (EARIST) with a President as its head. This made EARIST into a full-fledged College and authorized the establishment of Vocational-Technical School branches in each Congressional District of Manila.\\n- **Presidential Decree No. 1524**, signed by President Ferdinand E. Marcos on June 11, 1978, converted EARIST into a Chartered State College with Board of Trustees as its Governing Body and Dr. Hilario G. Nudas, as its first College President. In 1983, Dr. Frederick So Pada as SUC President followed by Dr. Lydia M. Profeta. Dr. Arturo P. Casuga is the 4th president followed by the 6th President Dr. Eduardo S. Caillao in 2006 to 2010. The 7th and current President of EARIST is Dr. Editha V. Pillo.\\n\\nToday, EARIST is:\\n- Baccalaureate College\\n- Comprehensive Teaching and Baccalaureate University LEVEL II\\n- Graduate Capable HEIs LEVEL III (Results of the CARNEGIE 2003 Classification Evaluation on the Typology of Philippine Higher Institution)\\n- Awarded LEVEL II STATUS in the Fourteen (14) Degrees Program (Accrediting Agency of Chartered Colleges and Universities in the Philippines)\\n- Rated SUC LEVEL II (CHED-DBM-PASUC Leveling Evaluation)',\n",
       "    'score': 0.5430461168289185,\n",
       "    'chunk_id': 0},\n",
       "   {'text': 'Board of Trustees\\nRepublic of the Philippines\\nEULOGIO \"AMANG\" RODRIGUEZ\\nINSTITUTE OF SCIENCE AND TECHNOLOGY\\nNagtahan, Sampoc, Manila\\n\\nBOARD OF TRUSTEES\\n(As of May 2021)\\n\\nHON. RONALD L. ADAMAT\\nCommissioner, CHED\\nChairperson ‚Äì Designate, EARIST BOT\\nHON. EDITHA V. PILLO\\nPresident, EARIST\\nVice ‚Äì Chair\\nHON. MARK O. GO\\nChairperson, House Committee on Higher and Technical Education\\nHON. SEN. JOEL J. VILLANUEVA\\nChairman, Senate Committee on Education, Arts, and Culture\\nHON. JOSE B. PATALINJUG III\\nDirector, DOST-NCR\\nMember\\nHON. CAPT. JOHNNY G. YU\\nPrivate Sector Representative\\nMember\\nHON. REYNALDO R. CANCIO\\nDirector, NPPS ‚Äì NEDA\\nMember\\nHON. ROGELIO T. MAMARADLO\\nPresident, EARIST Alumni Association, Inc.\\nMember\\nHON. EMMANUEL ALADIN D. TUMANDA\\nPrivate Sector Representative\\nMember\\nHON. BONIEBERT P. LUCIANO\\nPresident, EARIST-ISG Federated\\nMember\\nMS. GILDA S. FAMILARA\\nCollege and Board Secretary',\n",
       "    'score': 0.536401629447937,\n",
       "    'chunk_id': 110},\n",
       "   {'text': 'Chairman, EARIST Grievance Committee and College Legal Counsel\\nMS. GILDA S. FAMILARA\\nCollege & Board Secretary\\nCHAIR\\nDR. FREDERICK C. PENA\\nVice President for Academic Affairs\\nCo-Chair\\nMS. AGNES F. AMORIN\\nDirector, Student Affairs and Services\\nFacilitators\\nMR. VIRGEL E. DIAMANTE\\nChief, Publication and Yearbook\\nMR. RAYMOND DP. CANSINO\\nLeadership Coordinator\\nMR. MARTIN T. AGUILAR\\nScholarship Coordinator\\nMR. LAREX TAGALOG\\nFaculty',\n",
       "    'score': 0.5018723011016846,\n",
       "    'chunk_id': 116},\n",
       "   {'text': '## ARTICLE V\\n### RIGHTS AND RESPONSIBILITIES OF STUDENTS\\n\\n#### Section 1. STUDENT RIGHTS\\n\\nBonafide students of EARIST shall enjoy the following rights:\\n\\n##### 1.1 To choose a field of study subject to fair, reasonable, and equitable admission and academic requirements;\\n##### 1.2 To participate in matters affecting their welfare and to establish development for their future;\\n##### 1.3 To avail of career guidance and counseling services;\\n##### 1.4 To access and receive their official certificates, diplomas, transcript of records, report of grades, transfer credentials, and similar documents upon request and payment of corresponding fees;\\n##### 1.5 To publish a student newspaper and similar publications consistent with the provisions of the \"Campus Journalism Act of 1991\";\\n##### 1.6 To privacy of communication and correspondence to defend the rights of the students to freedom of speech and of expression;\\n##### 1.7 To take part in duly approved projects and activities to foster holistic growth and development of the studentry and other EARISTians;\\n##### 1.8 To freely express one\\'s gender preference to follow the norms of the Institution without due prejudice to any provisions of this handbook;\\n##### 1.9 To seek justification on contested grades imposed by any faculty. In such a case, a student may file a re-computation/rectification in writing within a prescribed period through the Dean/ Campus Administrator (for major courses) or the Director of Instruction (for general education courses);',\n",
       "    'score': 0.4890276789665222,\n",
       "    'chunk_id': 47},\n",
       "   {'text': 'Republic of the Philippines\\nEULOGIO \"AMANG\" RODRIGUEZ\\nINSTITUTE OF SCIENCE AND TECHNOLOGY\\nNagtahan, Sampoc, Manila\\n\\nINSTITUTE OFFICIAL\\n\\nDR. EDITHA V. PILLO\\nPresident\\nDR. FREDERICK C. PENA\\nVice President for Academic Affairs\\nDR. GRANT B. CORNELL\\nVice President for Production, Research, Extension and International Affairs\\nAR. FERNANDO DP. PAMINTUAN\\nDean, College of Architecture and Fine Arts\\nDR. RAYMUND B. BOLALIN\\nDean, College of Arts and Sciences\\nDR. WILLY O. GAPASIN\\nDean, College of Business and Public Administration\\nDR. ELEONOR T. SALVADOR\\nDean, College of Education\\nENGR. APOLINARIO S. SOLLANO\\nDean, College of Engineering\\nDR. JOSIE R. SONIO\\nDean, College of Industrial Technology\\nPROF. MARIA RHODA D. DINAGA\\nDean, College of Hospitality and Tourism Management\\nDR. ANABEL D. RIVA\\nDean, College of Criminal Justice Education\\nDR. MARLENE M. MONTERONA\\nDean, Graduate School',\n",
       "    'score': 0.48648980259895325,\n",
       "    'chunk_id': 111}],\n",
       "  'context_used': '[Section 1] HISTORY OF EARIST\\n\\nThe Eulogio \"Amang\" Rodriguez Institute of Science and Technology (EARIST) was established after the liberation of Manila in 1945. EARIST traces back its development from Vocational High School with only a room at the second floor of the Mapa High School, nine teachers, a clerk, and 147 students under Mr. Pantaleon Regala, its principal. Its former name was Eulogio Rodriguez Vocational High School (ERVHS).\\n\\nOn July 1, 1946, EARIST acquired its present site at Nagtahan, Sampaloc, Manila. Mr. Apolinario Apilado was appointed as principal. He was succeeded by Dr. Hilario G. Nudas in 1949.\\n\\nEARIST\\'s growth and development were made possible via three Republic Acts and Presidential Decree, to wit:\\n\\n- **Republic Act No. 4072**, jointly sponsored by Congressman Ramon D. Bagatsing and Salih Ujutalam in 1964, authorized the establishment of the Technical Education Department without changing the name of the school. It was headed by a Vocational Director.\\n- **Republic Act No. 5088**, sponsored by the late Congressman Sergio Loyola in 1967, authorized the renaming of ERVHS to Eulogio \"Amang\" Rodriguez Memorial School of Arts and Trades (EARMSAT) and signaled its',\n",
       "  'timestamp': '2025-10-26 19:58:00',\n",
       "  'response_time': 33.85761070251465},\n",
       " {'question': 'What is CAS College',\n",
       "  'response': 'The College of Arts and Sciences (CAS) is a college within the General Alvarez School of Arts and Trades (EARIST) located in EARIST Cavite Campus, General Mariano Alvarez, Cavite. The college offers several undergraduate degree programs, including Bachelor of Science in Applied Physics with Computer Science Emphasis (BSAP), Bachelor of Science in Computer Science (BSCS), Bachelor of Science in Information Technology (BS INFO. TECH.), Bachelor of Science in Psychology (BSPSYCH), and Bachelor of Science in Mathematics (BSMATH). These programs are awarded Level II status by the Accrediting Agency of Chartered Colleges and Universities in the Philippines (SUC Level II) and are also rated SUC Level II (CHED-DBM-PASUC Leveling Evaluation).',\n",
       "  'retrieved_chunks': [{'text': 'CURRICULAR OFFERINGS\\n(Approved Changes in Nomenclature)\\n\\n### MAIN CAMPUS\\n\\n#### A. COLLEGE OF ARCHITECTURE AND FINE ARTS (CAFA)\\n- Bachelor of Science in Architecture (BS ARCHI.)\\n- Bachelor of Science in Interior Design (BSID)\\n- Bachelor of Fine Arts (BFA)\\n  - Major in:\\n    - Painting\\n    - Visual Communication\\n\\n#### B. COLLEGE OF ARTS AND SCIENCES (CAS)\\n- Bachelor of Science in Applied Physics with Computer Science Emphasis (BSAP)\\n- Bachelor of Science in Computer Science (BSCS)\\n- Bachelor of Science in Information Technology (BS INFO. TECH.)\\n- Bachelor of Science in Psychology (BSPSYCH)\\n- Bachelor of Science in Mathematics (BSMATH)',\n",
       "    'score': 0.4806593656539917,\n",
       "    'chunk_id': 3},\n",
       "   {'text': '- Awarded LEVEL II STATUS in the Fourteen (14) Degrees Program (Accrediting Agency of Chartered Colleges and Universities in the Philippines)\\n- Rated SUC LEVEL II (CHED-DBM-PASUC Leveling Evaluation)\\n\\nHISTORY OF (GENERAL ALVAREZ SCHOOL OF ARTS AND TRADES)\\nEARIST Cavite Campus\\nGeneral Mariano Alvarez, Cavite\\n\\nEARIST PRESIDENT\\'S PART ON THE DEVELOPMENT OF EARIST ‚Äì CAVITE CAMPUS\\n\\nThe following are the hallmark of development during the tenure of the following EARIST Presidents:\\n\\nDR. HILARIO G. NUDAS ‚Äì 1972 ‚Äì 1982 (College President)\\n1978 ‚Äì 1982 (SUC President)\\n\\n- Establishment of EARIST ‚Äì GASAT (now EARIST ‚Äì Cavite Campus ‚Äì ECC) through board resolution No. 007-82 on March 24, 1982, approving the establishment and opening of GASAT as EARIST Branch in General Mariano Alvarez, Cavite. The funding support came from the budgetary allotment of EARIST.\\n\\nConstruction of EARIST ‚Äì Cavite Campus school buildings\\n\\n1. December 29, 1983 ‚Äì Acquisition of 31,282 sq. meter lot exclusively for GASAT school site donated by the National Housing Authority.\\n2. December 8-10, 1988 ‚Äì Relocation Survey of boundary lines of the 31,282 sq. meter lot.\\n3. March 18, 1989 ‚Äì Laying of the cornerstone for GASAT main building.\\n4. March 2, 1990 ‚Äì Groundbreaking ceremony led by Dr. Lydia Macaraig Profeta. This started the construction of GASAT main school building (Phase 1) with an initial budget allocation of 5M Pesos from the congressional insertions of Congressman RENATO P. DRAGON, then representative of the 2nd District of Cavite.\\n5. In one decade (1989-1999), the school building and infrastructure projects (Phase I-X) was completed with a total cost of PHP 42,682,197.40 all from the congressional insertions of Hon. RENATO P. DRAGON.\\n6. Finished Phase I (June 14, 1999) of the Multi-Purpose Sports Complex Gymnasium with a 2.5M appropriation from Speaker Jose C. De Venecia, Jr.\\n7. Completed the construction/improvement of one (1) school building with an appropriation of 1M from Congressman Erineo \"Ayong\" Maliksi on January 2000.',\n",
       "    'score': 0.4000709354877472,\n",
       "    'chunk_id': 1},\n",
       "   {'text': '- Doctor in Business Administration\\n- Doctor in Public Administration\\n\\n\\nEARIST - CAVITE CAMPUS\\n\\n#### J. Graduate Program\\n- Doctor in Education\\n  - Major in: Educational Management\\n- Master of Arts in Education\\n  - Major in: Administration and Supervision\\n- Master in Business Administration\\n\\n#### K. Post Baccalaureate Program\\n- Professional Education / Subjects (18 units)\\n\\n#### L. Undergraduate Program\\n- Bachelor of Science in Business Administration\\n  - Major in: Marketing Management\\n- Bachelor of Science in Computer Science\\n- Bachelor of Science in Computer Technology\\n- Bachelor of Science in Criminology\\n- Bachelor of Science in Hospitality Management\\n- Bachelor of Science in Office Administration\\n- Bachelor of Science in Industrial Psychology\\n- Bachelor of Technology and Livelihood Education\\n- Bachelor of Science in Industrial Technology\\n  - Major in:\\n    - Automotive\\n    - Electrical\\n    - Electronics\\n    - Food Technology\\n    - Drafting Technology',\n",
       "    'score': 0.39503657817840576,\n",
       "    'chunk_id': 8},\n",
       "   {'text': '#### H. COLLEGE OF CRIMINAL JUSTICE EDUCATION\\n- Bachelor of Science in Criminology (BSCrim)\\n\\n#### I. GRADUATE SCHOOL\\n- Master of Science in Mathematics\\n- Master of Arts in Industrial Psychology\\n- Master in Business Administration\\n- Master in Public Administration\\n- Master of Arts in Industrial Education\\n  - Major in:\\n    - Hotel Management\\n- Master of Arts in Education\\n  - Major in:\\n    - Administration and Supervision\\n    - Guidance & Counseling\\n    - Special Education\\n- Master of Arts in Teaching\\n  - Major in:\\n    - Electronics Technology\\n    - Mathematics\\n    - Science\\n- Doctor of Philosophy - Industrial Psychology\\n- Doctor of Education - Educational Management\\n- Doctor in Business Administration\\n- Doctor in Public Administration',\n",
       "    'score': 0.3770161271095276,\n",
       "    'chunk_id': 7},\n",
       "   {'text': \"- A notarized affidavit of support and proof of adequate financial support\\n- Equivalent High School Diploma\\n\\n\\nSCREENING PROCESS\\n\\nThere shall be three levels of screening. These are the following:\\n\\nScreening Level 1: EARIST College Admission Test (EARISTCAT)\\nThe transmutation formula for the EARIST-CAT scores of the applicants in any program shall be:\\n\\nEARISTCAT Rating = (Total Score x 50/No. Of Items) +50\\n\\nEARISTCAT applications shall be processed at the office of the Student Admission, Registration and Records Management Services (SARRMS) while EARIST College Admission Test shall be administered by the Guidance and Counselling Office of the Student Affairs Services (SAS) in coordination with Management Information System and Technology Services (MISTS).\\n\\nScreening Level 2: College Admission Interview\\nThe College Admission Interview is intended to help the institution assess the student as a candidate. The interviewer, who is the chairperson or a faculty of the program that the student is applying for, speaks with the student, takes notes, and evaluates the student's leadership potential, determination and perseverance, career choice, and communication skills.\\n\\nFor Architecture and Fine Arts applicants, a Drawing Ability Test (DAT) will be conducted as part of the screening process.\\n\\nEach college has the prerogative to define their own mechanism and/or evaluation rubric in the assessment of the student's performance in the interview.\\n\\nThe evaluation of the interviewer is final and unappealable.\",\n",
       "    'score': 0.36817753314971924,\n",
       "    'chunk_id': 11}],\n",
       "  'context_used': \"[Section 1] CURRICULAR OFFERINGS\\n(Approved Changes in Nomenclature)\\n\\n### MAIN CAMPUS\\n\\n#### A. COLLEGE OF ARCHITECTURE AND FINE ARTS (CAFA)\\n- Bachelor of Science in Architecture (BS ARCHI.)\\n- Bachelor of Science in Interior Design (BSID)\\n- Bachelor of Fine Arts (BFA)\\n  - Major in:\\n    - Painting\\n    - Visual Communication\\n\\n#### B. COLLEGE OF ARTS AND SCIENCES (CAS)\\n- Bachelor of Science in Applied Physics with Computer Science Emphasis (BSAP)\\n- Bachelor of Science in Computer Science (BSCS)\\n- Bachelor of Science in Information Technology (BS INFO. TECH.)\\n- Bachelor of Science in Psychology (BSPSYCH)\\n- Bachelor of Science in Mathematics (BSMATH)\\n\\n[Section 2] - Awarded LEVEL II STATUS in the Fourteen (14) Degrees Program (Accrediting Agency of Chartered Colleges and Universities in the Philippines)\\n- Rated SUC LEVEL II (CHED-DBM-PASUC Leveling Evaluation)\\n\\nHISTORY OF (GENERAL ALVAREZ SCHOOL OF ARTS AND TRADES)\\nEARIST Cavite Campus\\nGeneral Mariano Alvarez, Cavite\\n\\nEARIST PRESIDENT'S PART ON THE DEVELOPMENT OF EARIST ‚Äì CAVITE CAMPUS\\n\\nThe following are the hallmark of development during the tenure of the following EARIST Presidents:\\n\\nDR. HILARIO G. NUDAS ‚Äì 1972 ‚Äì 1982 (College President)\",\n",
       "  'timestamp': '2025-10-26 19:59:45',\n",
       "  'response_time': 80.81784296035767}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def interactive_rag_chat():\n",
    "    \"\"\"Enhanced interactive chat interface optimized for RTX 5050\"\"\"\n",
    "    if not model_loaded:\n",
    "        print(\"‚ùå Cannot start chat - model not loaded\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ü§ñ INTERACTIVE RAG CHAT - RTX 5050 OPTIMIZED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Ask questions about the student handbook!\")\n",
    "    print(\"\\nüìã Commands:\")\n",
    "    print(\"  'quit' or 'exit' - End chat\")\n",
    "    print(\"  'help' - Show commands\")\n",
    "    print(\"  'memory' - Show GPU memory usage\")\n",
    "    print(\"  'history' - Show chat history\")\n",
    "    print(\"  'clear' - Clear chat history\")\n",
    "    print(\"  'save' - Save chat history to file\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    chat_history = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Show memory status\n",
    "            if torch.cuda.is_available():\n",
    "                memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "                memory_percent = (memory_used / 8) * 100\n",
    "                status = \"üü¢\" if memory_percent < 80 else \"üü°\" if memory_percent < 90 else \"üî¥\"\n",
    "                print(f\"\\n{status} GPU Memory: {memory_used:.1f}GB ({memory_percent:.0f}%)\")\n",
    "            \n",
    "            user_input = input(\"\\nüí¨ Your question: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Chat ended. Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            elif user_input.lower() == 'help':\n",
    "                print(\"\\nüìã Available Commands:\")\n",
    "                print(\"  help - Show this help\")\n",
    "                print(\"  quit/exit/q - End chat\")\n",
    "                print(\"  memory - Show detailed GPU memory info\")\n",
    "                print(\"  history - Show chat history\")\n",
    "                print(\"  clear - Clear chat history\")\n",
    "                print(\"  save - Save chat history to file\")\n",
    "                print(\"  settings - Show current RAG settings\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'memory':\n",
    "                if torch.cuda.is_available():\n",
    "                    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "                    memory_free = 8 - memory_reserved\n",
    "                    print(f\"\\nüîç GPU Memory Status:\")\n",
    "                    print(f\"  Allocated: {memory_allocated:.2f} GB\")\n",
    "                    print(f\"  Reserved:  {memory_reserved:.2f} GB\")\n",
    "                    print(f\"  Free:      {memory_free:.2f} GB\")\n",
    "                    print(f\"  Total:     8.0 GB\")\n",
    "                else:\n",
    "                    print(\"‚ùå CUDA not available\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'settings':\n",
    "                print(f\"\\n‚öôÔ∏è Current RAG Settings:\")\n",
    "                print(f\"  Temperature: {RAG_CONFIG['generation']['temperature']}\")\n",
    "                print(f\"  Top-p: {RAG_CONFIG['generation']['top_p']}\")\n",
    "                print(f\"  Max new tokens: 256 (optimized for RTX 5050)\")\n",
    "                print(f\"  Retrieval top-k: {RAG_CONFIG['retrieval']['top_k']}\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'history':\n",
    "                if chat_history:\n",
    "                    print(f\"\\nüìú Chat History ({len(chat_history)} questions):\")\n",
    "                    for i, item in enumerate(chat_history, 1):\n",
    "                        print(f\"  {i}. {item['question'][:60]}{'...' if len(item['question']) > 60 else ''}\")\n",
    "                else:\n",
    "                    print(\"\\nüìú No chat history yet.\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'clear':\n",
    "                chat_history.clear()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                print(\"üóëÔ∏è Chat history cleared and GPU cache cleaned.\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'save':\n",
    "                if chat_history:\n",
    "                    filename = f\"chat_history_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump([{\n",
    "                            'question': item['question'],\n",
    "                            'response': item['response'],\n",
    "                            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        } for item in chat_history], f, indent=2)\n",
    "                    print(f\"üíæ Chat history saved to {filename}\")\n",
    "                else:\n",
    "                    print(\"üìú No chat history to save.\")\n",
    "                continue\n",
    "            \n",
    "            elif not user_input:\n",
    "                print(\"‚ùì Please enter a question.\")\n",
    "                continue\n",
    "            \n",
    "            # Generate response with timing\n",
    "            start_time = time.time()\n",
    "            result = rag_system.generate_response(user_input, max_new_tokens=256, verbose=True)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            result['timestamp'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            result['response_time'] = elapsed_time\n",
    "            chat_history.append(result)\n",
    "            \n",
    "            print(f\"\\n‚è±Ô∏è Response time: {elapsed_time:.1f}s\")\n",
    "            \n",
    "            # Memory cleanup after each response\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n‚ö†Ô∏è Chat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return chat_history\n",
    "\n",
    "# Start interactive chat (uncomment to use)\n",
    "# chat_history = interactive_rag_chat()\n",
    "\n",
    "print(\"üöÄ Enhanced interactive chat function ready!\")\n",
    "\n",
    "interactive_rag_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ff08040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Advanced batch processing functions ready!\n",
      "\n",
      "Usage examples:\n",
      "  batch_process_test_questions()  # Process 8 test questions\n",
      "  batch_process_questions_json()  # Process all Questions.json\n",
      "  batch_process_questions(['Q1', 'Q2'], 'results.json')  # Custom questions\n"
     ]
    }
   ],
   "source": [
    "def batch_process_questions(questions, output_file=None, memory_cleanup_interval=5):\n",
    "    \"\"\"\n",
    "    Batch process multiple questions with memory optimization for RTX 5050\n",
    "    \n",
    "    Args:\n",
    "        questions: List of questions (strings) or list of dicts with 'question' key\n",
    "        output_file: Optional filename to save results\n",
    "        memory_cleanup_interval: Clean GPU memory every N questions\n",
    "    \"\"\"\n",
    "    if not model_loaded:\n",
    "        print(\"‚ùå Cannot process - model not loaded\")\n",
    "        return None\n",
    "    \n",
    "    # Handle different input formats\n",
    "    if isinstance(questions, str):\n",
    "        questions = [questions]\n",
    "    \n",
    "    question_list = []\n",
    "    for q in questions:\n",
    "        if isinstance(q, str):\n",
    "            question_list.append(q)\n",
    "        elif isinstance(q, dict) and 'question' in q:\n",
    "            question_list.append(q['question'])\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping invalid question format: {q}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ BATCH PROCESSING MODE - RTX 5050 OPTIMIZED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìä Processing {len(question_list)} questions\")\n",
    "    print(f\"üßπ Memory cleanup every {memory_cleanup_interval} questions\")\n",
    "    print(f\"üíæ Output file: {output_file or 'None (in-memory only)'}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, question in enumerate(question_list, 1):\n",
    "        print(f\"\\nüìù Processing {i}/{len(question_list)}: {question[:60]}{'...' if len(question) > 60 else ''}\")\n",
    "        \n",
    "        # Memory status before processing\n",
    "        if torch.cuda.is_available():\n",
    "            memory_before = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"   üíæ GPU Memory: {memory_before:.1f}GB\")\n",
    "        \n",
    "        try:\n",
    "            # Generate response with reduced verbosity for batch mode\n",
    "            question_start = time.time()\n",
    "            result = rag_system.generate_response(\n",
    "                question, \n",
    "                max_new_tokens=256, \n",
    "                verbose=False  # Reduce output for batch processing\n",
    "            )\n",
    "            question_time = time.time() - question_start\n",
    "            \n",
    "            # Add metadata\n",
    "            result.update({\n",
    "                'batch_index': i,\n",
    "                'processing_time': question_time,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'gpu_memory_before': memory_before if torch.cuda.is_available() else None\n",
    "            })\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"   ‚úÖ Completed in {question_time:.1f}s\")\n",
    "            print(f\"   üìã Found {len(result['retrieved_chunks'])} relevant chunks\")\n",
    "            \n",
    "            # Memory cleanup at intervals\n",
    "            if i % memory_cleanup_interval == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    memory_after = torch.cuda.memory_allocated() / 1e9\n",
    "                    print(f\"   üßπ Memory cleaned: {memory_after:.1f}GB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing question {i}: {e}\")\n",
    "            error_result = {\n",
    "                'question': question,\n",
    "                'response': f\"Error: {str(e)}\",\n",
    "                'retrieved_chunks': [],\n",
    "                'context_used': \"\",\n",
    "                'batch_index': i,\n",
    "                'processing_time': 0,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'error': str(e)\n",
    "            }\n",
    "            results.append(error_result)\n",
    "            \n",
    "            # Force memory cleanup on error\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Final cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    successful_results = [r for r in results if 'error' not in r]\n",
    "    error_count = len(results) - len(successful_results)\n",
    "    avg_time = np.mean([r['processing_time'] for r in successful_results]) if successful_results else 0\n",
    "    total_chunks = sum(len(r['retrieved_chunks']) for r in successful_results)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä BATCH PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"‚úÖ Successful: {len(successful_results)}/{len(question_list)}\")\n",
    "    print(f\"‚ùå Errors: {error_count}\")\n",
    "    print(f\"‚è±Ô∏è Total time: {total_time:.1f}s\")\n",
    "    print(f\"‚ö° Average time per question: {avg_time:.1f}s\")\n",
    "    print(f\"üìã Total chunks retrieved: {total_chunks}\")\n",
    "    print(f\"üß† Average chunks per question: {total_chunks/len(successful_results) if successful_results else 0:.1f}\")\n",
    "    \n",
    "    # Save results if requested\n",
    "    if output_file:\n",
    "        try:\n",
    "            # Create serializable version\n",
    "            serializable_results = []\n",
    "            for result in results:\n",
    "                serialized = {\n",
    "                    'question': result['question'],\n",
    "                    'response': result['response'],\n",
    "                    'batch_index': result['batch_index'],\n",
    "                    'processing_time': result['processing_time'],\n",
    "                    'timestamp': result['timestamp'],\n",
    "                    'num_chunks_retrieved': len(result['retrieved_chunks']),\n",
    "                    'retrieval_scores': [chunk['score'] for chunk in result['retrieved_chunks']],\n",
    "                    'error': result.get('error', None)\n",
    "                }\n",
    "                serializable_results.append(serialized)\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'metadata': {\n",
    "                        'total_questions': len(question_list),\n",
    "                        'successful_results': len(successful_results),\n",
    "                        'error_count': error_count,\n",
    "                        'total_processing_time': total_time,\n",
    "                        'average_time_per_question': avg_time,\n",
    "                        'processing_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'model_config': RAG_CONFIG\n",
    "                    },\n",
    "                    'results': serializable_results\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Results saved to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    return results\n",
    "\n",
    "def load_questions_from_file(filepath):\n",
    "    \"\"\"Load questions from various file formats\"\"\"\n",
    "    try:\n",
    "        if filepath.endswith('.json'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    return data\n",
    "                elif isinstance(data, dict):\n",
    "                    # Handle Questions.json format\n",
    "                    questions = []\n",
    "                    for category, q_list in data.items():\n",
    "                        questions.extend(q_list)\n",
    "                    return questions\n",
    "        \n",
    "        elif filepath.endswith('.txt'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                return [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚ùå Unsupported file format: {filepath}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading questions from {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage functions\n",
    "def batch_process_test_questions():\n",
    "    \"\"\"Process the predefined test questions\"\"\"\n",
    "    test_questions = [\n",
    "        \"What are the admission requirements?\",\n",
    "        \"What is the grading system?\",\n",
    "        \"How do I apply for financial aid?\",\n",
    "        \"What are the library hours?\",\n",
    "        \"What is the academic calendar?\",\n",
    "        \"What are the graduation requirements?\",\n",
    "        \"How do I register for classes?\",\n",
    "        \"What academic support services are available?\"\n",
    "    ]\n",
    "    \n",
    "    return batch_process_questions(\n",
    "        test_questions, \n",
    "        output_file=f\"batch_test_results_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    )\n",
    "\n",
    "def batch_process_questions_json():\n",
    "    \"\"\"Process all questions from Questions.json file\"\"\"\n",
    "    questions = load_questions_from_file('../Retriever/Questions.json')\n",
    "    if questions:\n",
    "        return batch_process_questions(\n",
    "            questions, \n",
    "            output_file=f\"batch_all_questions_{time.strftime('%Y%m%d_%H%M%S')}.json\",\n",
    "            memory_cleanup_interval=3  # More frequent cleanup for larger batches\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ùå Could not load questions from Questions.json\")\n",
    "        return None\n",
    "\n",
    "print(\"üîÑ Advanced batch processing functions ready!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"  batch_process_test_questions()  # Process 8 test questions\")\n",
    "print(\"  batch_process_questions_json()  # Process all Questions.json\")\n",
    "print(\"  batch_process_questions(['Q1', 'Q2'], 'results.json')  # Custom questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f290c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions ready!\n"
     ]
    }
   ],
   "source": [
    "def evaluate_rag_with_questions_json():\n",
    "    \"\"\"Evaluate RAG system using your Questions.json file\"\"\"\n",
    "    if not model_loaded:\n",
    "        print(\"Cannot evaluate - model not loaded\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open('../Retriever/Questions.json', 'r', encoding='utf-8') as f:\n",
    "            questions_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Questions.json not found. Make sure it exists in ../Retriever/\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"RAG SYSTEM EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    all_results = []\n",
    "    total_questions = sum(len(questions) for questions in questions_data.values())\n",
    "    current_q = 0\n",
    "    \n",
    "    for category, questions in questions_data.items():\n",
    "        print(f\"\\nEvaluating Category: {category}\")\n",
    "        print(f\"Questions: {len(questions)}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        category_results = []\n",
    "        \n",
    "        for q_item in questions:\n",
    "            current_q += 1\n",
    "            question = q_item['question']\n",
    "            expected_ref = q_item['expected_reference']\n",
    "            \n",
    "            print(f\"\\n[{current_q}/{total_questions}] {question}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = rag_system.generate_response(question)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            result['expected_reference'] = expected_ref\n",
    "            result['category'] = category\n",
    "            result['elapsed_time'] = elapsed_time\n",
    "            \n",
    "            category_results.append(result)\n",
    "            all_results.append(result)\n",
    "            \n",
    "            print(f\"Time: {elapsed_time:.2f}s\")\n",
    "            print(\"-\" * 30)\n",
    "        \n",
    "        # Category summary\n",
    "        avg_time = np.mean([r['elapsed_time'] for r in category_results])\n",
    "        avg_chunks = np.mean([len(r['retrieved_chunks']) for r in category_results])\n",
    "        print(f\"\\nCategory Summary:\")\n",
    "        print(f\"  Average response time: {avg_time:.2f}s\")\n",
    "        print(f\"  Average chunks retrieved: {avg_chunks:.1f}\")\n",
    "    \n",
    "    # Overall summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total questions: {len(all_results)}\")\n",
    "    print(f\"Average response time: {np.mean([r['elapsed_time'] for r in all_results]):.2f}s\")\n",
    "    print(f\"Total evaluation time: {sum([r['elapsed_time'] for r in all_results]):.1f}s\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def save_evaluation_results(results, filename=\"rag_evaluation_results.json\"):\n",
    "    \"\"\"Save evaluation results to file\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to save\")\n",
    "        return\n",
    "    \n",
    "    # Convert to serializable format\n",
    "    serializable_results = []\n",
    "    for result in results:\n",
    "        serialized = {\n",
    "            'question': result['question'],\n",
    "            'response': result['response'],\n",
    "            'expected_reference': result.get('expected_reference', ''),\n",
    "            'category': result.get('category', ''),\n",
    "            'elapsed_time': result['elapsed_time'],\n",
    "            'num_chunks_retrieved': len(result['retrieved_chunks']),\n",
    "            'retrieval_scores': [chunk['score'] for chunk in result['retrieved_chunks']]\n",
    "        }\n",
    "        serializable_results.append(serialized)\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Evaluation results saved to {filename}\")\n",
    "\n",
    "# Run evaluation (uncomment to evaluate)\n",
    "# evaluation_results = evaluate_rag_with_questions_json()\n",
    "# save_evaluation_results(evaluation_results)\n",
    "\n",
    "print(\"Evaluation functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ QUICK START - Test Your RAG System on RTX 5050\n",
    "\n",
    "if model_loaded and rag_system is not None:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ TESTING RAG SYSTEM ON RTX 5050 (8GB VRAM)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Show current GPU memory status\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_percent = (memory_used / 8) * 100\n",
    "        \n",
    "        if memory_percent < 80:\n",
    "            status = \"\udfe2 Excellent\"\n",
    "        elif memory_percent < 90:\n",
    "            status = \"üü° Good\"\n",
    "        else:\n",
    "            status = \"üî¥ High\"\n",
    "        \n",
    "        print(f\"\\nüíæ GPU Memory: {memory_used:.1f} GB / 8 GB ({memory_percent:.0f}%) - {status}\")\n",
    "    \n",
    "    # Test with a sample question\n",
    "    sample_question = \"What are the admission requirements?\"\n",
    "    \n",
    "    print(f\"\\nüß™ Testing with sample question:\")\n",
    "    print(f\"'{sample_question}'\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Generate response with timing\n",
    "    start_time = time.time()\n",
    "    result = rag_system.generate_response(sample_question, max_new_tokens=200, verbose=True)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Response generated in {elapsed_time:.1f} seconds\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚úÖ SUCCESS! Your RAG system is working on RTX 5050!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Final memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        final_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"üßπ Memory after cleanup: {final_memory:.1f} GB / 8 GB\")\n",
    "    \n",
    "    # Usage guide\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\udcd6 HOW TO USE YOUR RAG SYSTEM\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n1Ô∏è‚É£  Ask a Single Question:\")\n",
    "    print(\"   result = rag_system.generate_response('your question here')\")\n",
    "    \n",
    "    print(\"\\n2Ô∏è‚É£  Interactive Chat Mode:\")\n",
    "    print(\"   interactive_rag_chat()\")\n",
    "    \n",
    "    print(\"\\n3Ô∏è‚É£  Batch Process Multiple Questions:\")\n",
    "    print(\"   batch_process_test_questions()\")\n",
    "    \n",
    "    print(\"\\n4Ô∏è‚É£  Check GPU Memory:\")\n",
    "    print(\"   torch.cuda.memory_allocated() / 1e9  # Shows GB used\")\n",
    "    print(\"   torch.cuda.empty_cache()             # Frees unused memory\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\udca1 TIPS FOR 8GB VRAM:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚úÖ Keep max_new_tokens ‚â§ 250 for safety\")\n",
    "    print(\"‚úÖ Process questions one at a time or use small batches\")\n",
    "    print(\"‚úÖ Run torch.cuda.empty_cache() if you get OOM errors\")\n",
    "    print(\"‚úÖ Monitor memory with the 'memory' command in interactive mode\")\n",
    "    print(\"‚úÖ Close other GPU-using applications for best performance\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ YOU'RE ALL SET! Start asking questions about the student handbook!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚ùå RAG SYSTEM NOT READY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nPlease run all cells in order:\")\n",
    "    print(\"1. ‚úÖ Cell 1: Import libraries and check GPU\")\n",
    "    print(\"2. ‚úÖ Cell 2: Load dataset\")\n",
    "    print(\"3. ‚úÖ Cell 3: Configure RAG settings\")\n",
    "    print(\"4. ‚úÖ Cell 4: Load/create chunks\")\n",
    "    print(\"5. ‚úÖ Cell 5: Initialize retriever\")\n",
    "    print(\"6. ‚úÖ Cell 6: Login to Hugging Face\")\n",
    "    print(\"7. ‚úÖ Cell 7: Load Mistral-7B model\")\n",
    "    print(\"8. ‚úÖ Cell 8: Initialize RAG generator\")\n",
    "    print(\"\\nThen run this cell again!\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e80f92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Function 'process_and_save_retrieved_chunks' is ready! Use it to process a question and save retrieved chunks to a .txt file.\n",
      "======================================================================\n",
      "Processing question: Who is the Current President of Earist?\n",
      "======================================================================\n",
      "\n",
      "‚ùì Question: Who is the Current President of Earist?\n",
      "======================================================================\n",
      "üîç Retrieving relevant context...\n",
      "üìö Found 5 relevant chunks:\n",
      "   1. Relevance score: 0.7226\n",
      "   2. Relevance score: 0.7042\n",
      "   3. Relevance score: 0.6187\n",
      "   4. Relevance score: 0.6060\n",
      "   5. Relevance score: 0.5853\n",
      "üíæ GPU memory before generation: 4.58 GB\n",
      "üìö Found 5 relevant chunks:\n",
      "   1. Relevance score: 0.7226\n",
      "   2. Relevance score: 0.7042\n",
      "   3. Relevance score: 0.6187\n",
      "   4. Relevance score: 0.6060\n",
      "   5. Relevance score: 0.5853\n",
      "üíæ GPU memory before generation: 4.58 GB\n",
      "üíæ GPU memory after cleanup: 4.58 GB\n",
      "\n",
      "üí° Answer:\n",
      "----------------------------------------------------------------------\n",
      "According to the handbook, the current President of EARIST is Dr. Editha V. Caillao.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üíæ Retrieved chunks saved to retrieved_chunks_Who_is_the_Current_President_of_Earist_20251023_151250.txt\n",
      "üíæ GPU memory after cleanup: 4.58 GB\n",
      "\n",
      "üí° Answer:\n",
      "----------------------------------------------------------------------\n",
      "According to the handbook, the current President of EARIST is Dr. Editha V. Caillao.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üíæ Retrieved chunks saved to retrieved_chunks_Who_is_the_Current_President_of_Earist_20251023_151250.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who is the Current President of Earist?',\n",
       " 'response': 'According to the handbook, the current President of EARIST is Dr. Editha V. Caillao.',\n",
       " 'retrieved_chunks': [{'text': 'Caillao in 2006 to 2010. The 7th and current President of EARIST is Dr. Editha V.',\n",
       "   'score': 0.7226303815841675,\n",
       "   'chunk_id': 43},\n",
       "  {'text': 'PILLO\\nPresident, EARIST\\nVice ‚Äì Chair\\nHON.',\n",
       "   'score': 0.7041721343994141,\n",
       "   'chunk_id': 447},\n",
       "  {'text': 'President ‚Äì the head of the school, college or university.',\n",
       "   'score': 0.6186845302581787,\n",
       "   'chunk_id': 587},\n",
       "  {'text': 'Elected officers...',\n",
       "   'score': 0.6059854030609131,\n",
       "   'chunk_id': 293},\n",
       "  {'text': 'Arturo P. Casuga is the 4th president followed by the 6th President Dr. Eduardo S.',\n",
       "   'score': 0.5853207111358643,\n",
       "   'chunk_id': 42}],\n",
       " 'context_used': '[Section 1] Caillao in 2006 to 2010. The 7th and current President of EARIST is Dr. Editha V.\\n\\n[Section 2] PILLO\\nPresident, EARIST\\nVice ‚Äì Chair\\nHON.\\n\\n[Section 3] President ‚Äì the head of the school, college or university.\\n\\n[Section 4] Elected officers...\\n\\n[Section 5] Arturo P. Casuga is the 4th president followed by the 6th President Dr. Eduardo S.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_and_save_retrieved_chunks(question, output_txt_file=None):\n",
    "    \"\"\"\n",
    "    Process a single question through the RAG workflow and save retrieved chunks to a .txt file.\n",
    "    Args:\n",
    "        question (str): The question to process.\n",
    "        output_txt_file (str): Path to the output .txt file. If None, uses a timestamped default.\n",
    "    Returns:\n",
    "        dict: The RAG response result.\n",
    "    \"\"\"\n",
    "    if not model_loaded or rag_system is None:\n",
    "        print(\"‚ùå RAG system not ready. Please ensure the model is loaded.\")\n",
    "        return None\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Processing question: {question}\")\n",
    "    print(\"=\" * 70)\n",
    "    result = rag_system.generate_response(question, verbose=True)\n",
    "\n",
    "    # Save retrieved chunks to .txt file\n",
    "    if output_txt_file is None:\n",
    "        safe_question = question[:40].replace(' ', '_').replace('?', '')\n",
    "        output_txt_file = f\"retrieved_chunks_{safe_question}_{time.strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "\n",
    "    try:\n",
    "        with open(output_txt_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Question: {question}\\n\\n\")\n",
    "            f.write(\"Retrieved Chunks:\\n\\n\")\n",
    "            for i, chunk in enumerate(result['retrieved_chunks'], 1):\n",
    "                f.write(f\"[Chunk {i}] (Score: {chunk['score']:.4f})\\n\")\n",
    "                f.write(chunk['text'] + \"\\n\\n\")\n",
    "        print(f\"\\nüíæ Retrieved chunks saved to {output_txt_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving retrieved chunks: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "print(\"\\nFunction 'process_and_save_retrieved_chunks' is ready! Use it to process a question and save retrieved chunks to a .txt file.\")\n",
    "\n",
    "\n",
    "process_and_save_retrieved_chunks(\"Who is the Current President of Earist?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
