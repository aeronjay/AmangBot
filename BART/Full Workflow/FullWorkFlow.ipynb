{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daad5b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "Transformers version: 4.57.1\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "GPU Memory: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f610c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 171,284 characters\n",
      "Estimated pages: ~85\n"
     ]
    }
   ],
   "source": [
    "with open('../Dataset/StudentHandbookDataset.txt', 'r', encoding='utf-8') as f:\n",
    "    dataset = f.read()\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset):,} characters\")\n",
    "print(f\"Estimated pages: ~{len(dataset) // 2000}\")\n",
    "\n",
    "# Import retrieval dependencies\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ca3a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Configuration:\n",
      "\n",
      "CHUNKING:\n",
      "  strategy: semantic\n",
      "  percentile_threshold: 50\n",
      "  buffer_size: 1\n",
      "\n",
      "EMBEDDING:\n",
      "  model_name: sentence-transformers/multi-qa-mpnet-base-dot-v1\n",
      "  device: cuda\n",
      "\n",
      "GENERATION:\n",
      "  model_name: mistralai/Mistral-7B-Instruct-v0.1\n",
      "  max_length: 2048\n",
      "  temperature: 0.3\n",
      "  top_p: 0.9\n",
      "  do_sample: True\n",
      "\n",
      "RETRIEVAL:\n",
      "  top_k: 5\n",
      "  score_threshold: 0.3\n"
     ]
    }
   ],
   "source": [
    "# RAG Configuration - Using your optimal settings from retrieval experiments\n",
    "RAG_CONFIG = {\n",
    "    'chunking': {\n",
    "        'strategy': 'semantic',\n",
    "        'percentile_threshold': 50,\n",
    "        'buffer_size': 1\n",
    "    },\n",
    "    'embedding': {\n",
    "        'model_name': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    },\n",
    "    'generation': {\n",
    "        'model_name': 'mistralai/Mistral-7B-Instruct-v0.1',\n",
    "        'max_length': 2048,\n",
    "        'temperature': 0.3,\n",
    "        'top_p': 0.9,\n",
    "        'do_sample': True\n",
    "    },\n",
    "    'retrieval': {\n",
    "        'top_k': 5,\n",
    "        'score_threshold': 0.3\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"RAG Configuration:\")\n",
    "for section, config in RAG_CONFIG.items():\n",
    "    print(f\"\\n{section.upper()}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3111c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing chunks from ../Retriever/saved_chunks/chunks_p50_b1.pkl\n",
      "Loaded 613 chunks\n",
      "\n",
      "Chunk Analysis:\n",
      "  Total chunks: 613\n",
      "  Average size: 260 characters\n",
      "  Size range: 2 - 4752 characters\n"
     ]
    }
   ],
   "source": [
    "def load_or_create_chunks():\n",
    "    \"\"\"Load existing chunks or create new ones with optimal config\"\"\"\n",
    "    chunk_filename = f\"../Retriever/saved_chunks/chunks_p{RAG_CONFIG['chunking']['percentile_threshold']}_b{RAG_CONFIG['chunking']['buffer_size']}.pkl\"\n",
    "    \n",
    "    if os.path.exists(chunk_filename):\n",
    "        print(f\"Loading existing chunks from {chunk_filename}\")\n",
    "        with open(chunk_filename, 'rb') as f:\n",
    "            chunks = pickle.load(f)\n",
    "        print(f\"Loaded {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "    \n",
    "    print(\"Creating new chunks with optimal configuration...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # Lightweight for chunking\n",
    "        model_kwargs={'device': RAG_CONFIG['embedding']['device']},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings=embedding_model,\n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "        breakpoint_threshold_amount=RAG_CONFIG['chunking']['percentile_threshold'],\n",
    "        buffer_size=RAG_CONFIG['chunking']['buffer_size'],\n",
    "        add_start_index=True\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.create_documents([dataset])\n",
    "    \n",
    "    # Save chunks\n",
    "    os.makedirs(\"../Retriever/saved_chunks\", exist_ok=True)\n",
    "    with open(chunk_filename, 'wb') as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    \n",
    "    print(f\"Created and saved {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "chunks = load_or_create_chunks()\n",
    "\n",
    "# Analyze chunks\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "print(f\"\\nChunk Analysis:\")\n",
    "print(f\"  Total chunks: {len(chunks)}\")\n",
    "print(f\"  Average size: {np.mean(chunk_sizes):.0f} characters\")\n",
    "print(f\"  Size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e14d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing retrieval system...\n",
      "Setting up embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tebats\\AppData\\Local\\Temp\\ipykernel_20800\\2777203703.py:14: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embedding_model = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for all chunks...\n",
      "  Processed 320/613 chunks\n",
      "  Processed 613/613 chunks\n",
      "Building FAISS index...\n",
      "Retrieval system ready: 613 vectors (768D)\n",
      "Retrieval system ready!\n"
     ]
    }
   ],
   "source": [
    "class OptimalRetriever:\n",
    "    \"\"\"Retrieval system using your best-performing configuration\"\"\"\n",
    "    \n",
    "    def __init__(self, chunks, config):\n",
    "        self.chunks = chunks\n",
    "        self.config = config\n",
    "        self.embedding_model = None\n",
    "        self.index = None\n",
    "        self.setup_embeddings()\n",
    "    \n",
    "    def setup_embeddings(self):\n",
    "        \"\"\"Initialize embedding model and FAISS index\"\"\"\n",
    "        print(\"Setting up embedding model...\")\n",
    "        self.embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=self.config['embedding']['model_name'],\n",
    "            model_kwargs={\n",
    "                'device': self.config['embedding']['device'],\n",
    "                'trust_remote_code': True   \n",
    "            },\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        print(\"Generating embeddings for all chunks...\")\n",
    "        chunk_texts = [chunk.page_content for chunk in self.chunks]\n",
    "        \n",
    "        # Batch processing to avoid memory issues\n",
    "        batch_size = 32\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in range(0, len(chunk_texts), batch_size):\n",
    "            batch = chunk_texts[i:i+batch_size]\n",
    "            batch_embeddings = self.embedding_model.embed_documents(batch)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i + len(batch)}/{len(chunk_texts)} chunks\")\n",
    "        \n",
    "        # Build FAISS index\n",
    "        print(\"Building FAISS index...\")\n",
    "        dimension = len(all_embeddings[0])\n",
    "        self.index = faiss.IndexFlatIP(dimension)\n",
    "        \n",
    "        embeddings_array = np.array(all_embeddings).astype('float32')\n",
    "        faiss.normalize_L2(embeddings_array)\n",
    "        self.index.add(embeddings_array)\n",
    "        \n",
    "        print(f\"Retrieval system ready: {self.index.ntotal:,} vectors ({dimension}D)\")\n",
    "    \n",
    "    def retrieve(self, query, top_k=None, score_threshold=None):\n",
    "        \"\"\"Retrieve relevant chunks for a query\"\"\"\n",
    "        top_k = top_k or self.config['retrieval']['top_k']\n",
    "        score_threshold = score_threshold or self.config['retrieval']['score_threshold']\n",
    "        \n",
    "        # Embed query\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        query_vector = np.array([query_embedding]).astype('float32')\n",
    "        faiss.normalize_L2(query_vector)\n",
    "        \n",
    "        # Search\n",
    "        scores, indices = self.index.search(query_vector, top_k)\n",
    "        \n",
    "        # Filter by threshold and format results\n",
    "        relevant_chunks = []\n",
    "        for idx, score in zip(indices[0], scores[0]):\n",
    "            if score >= score_threshold and idx < len(self.chunks):\n",
    "                relevant_chunks.append({\n",
    "                    'text': self.chunks[idx].page_content,\n",
    "                    'score': float(score),\n",
    "                    'chunk_id': int(idx)\n",
    "                })\n",
    "        \n",
    "        return relevant_chunks\n",
    "\n",
    "# Initialize retriever\n",
    "print(\"Initializing retrieval system...\")\n",
    "retriever = OptimalRetriever(chunks, RAG_CONFIG)\n",
    "print(\"Retrieval system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8043d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Logging in to Hugging Face...\n",
      "You'll be prompted to enter your HF token.\n",
      "Get your token from: https://huggingface.co/settings/tokens\n",
      "\n",
      "Note: The token input will be hidden for security.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe48d14d6fb4c89b04d2e7f59d8e4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face to download Mistral-7B\n",
    "print(\"üîê Logging in to Hugging Face...\")\n",
    "print(\"You'll be prompted to enter your HF token.\")\n",
    "print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
    "print(\"\\nNote: The token input will be hidden for security.\")\n",
    "\n",
    "try:\n",
    "    login()\n",
    "    print(\"‚úÖ Successfully logged in to Hugging Face!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Login failed: {e}\")\n",
    "    print(\"\\nAlternative: You can set your token in environment variable:\")\n",
    "    print(\"HF_TOKEN=your_token_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17bddb",
   "metadata": {},
   "source": [
    "## üîê Step 6: Hugging Face Authentication\n",
    "\n",
    "Before downloading Mistral-7B, you need to authenticate with Hugging Face. This is required to access gated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d83583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Still working... please wait...\n",
      "üöÄ LOADING MISTRAL-7B FOR RTX 5050 (8GB VRAM)\n",
      "======================================================================\n",
      "GPU: NVIDIA GeForce RTX 5050 Laptop GPU\n",
      "Total VRAM: 8.5 GB\n",
      "Initial GPU memory: 0.45 GB\n",
      "\n",
      "üìù Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\tokenizer.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded successfully\n",
      "\n",
      "‚öôÔ∏è Configuring 4-bit quantization for 8GB VRAM...\n",
      "‚úÖ Quantization config ready (4-bit NF4 + double quant)\n",
      "\n",
      "üîÑ Loading mistralai/Mistral-7B-Instruct-v0.1 with 4-bit quantization...\n",
      "üì¶ This will download ~13GB if not cached (may take 5-10 minutes)\n",
      "‚è≥ Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\config.json\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": null,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\model.safetensors.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfb0e5417194298b2fc62196aaedd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at C:\\Users\\tebats\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-Instruct-v0.1\\snapshots\\ec5deb64f2c6e6fa90c1abf74a91d5c93a9669ca\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside mistralai/Mistral-7B-Instruct-v0.1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ MODEL LOADED SUCCESSFULLY!\n",
      "======================================================================\n",
      "üíæ GPU Memory Status:\n",
      "   Allocated:  4.58 GB\n",
      "   Reserved:   4.70 GB\n",
      "   Free:       3.30 GB\n",
      "   Total:      8.00 GB\n",
      "   Usage:      58.8%\n",
      "   Status:     üü¢ Excellent - Plenty of headroom\n",
      "\n",
      "üéØ Optimizations Applied for 8GB VRAM:\n",
      "   ‚úÖ 4-bit NF4 quantization (~4GB model size)\n",
      "   ‚úÖ Double quantization for extra savings\n",
      "   ‚úÖ Float16 compute dtype\n",
      "   ‚úÖ Automatic device mapping\n",
      "   ‚úÖ Low CPU memory usage mode\n",
      "\n",
      "üìä Model Configuration:\n",
      "   Model:       mistralai/Mistral-7B-Instruct-v0.1\n",
      "   Precision:   4-bit (quantized from 16-bit)\n",
      "   Device:      cuda:0\n",
      "   Parameters:  ~7B (quantized)\n",
      "\n",
      "======================================================================\n",
      "üèÅ Final Status: ‚úÖ READY TO USE\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n",
      "‚è≥ Still working... please wait...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import asyncio\n",
    "asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "# ==========================================================\n",
    "# Optimized Model Loading for RTX 5050 (8GB VRAM) ‚Äî with Live Output\n",
    "# ==========================================================\n",
    "import sys, os, time, threading\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.utils import logging\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# === Enable live print output and progress ===\n",
    "os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "logging.set_verbosity_info()\n",
    "logging.enable_progress_bar()\n",
    "\n",
    "def heartbeat():\n",
    "    while True:\n",
    "        print(\"‚è≥ Still working... please wait...\", flush=True)\n",
    "        time.sleep(60)\n",
    "\n",
    "threading.Thread(target=heartbeat, daemon=True).start()\n",
    "\n",
    "# ==========================================================\n",
    "print(\"üöÄ LOADING MISTRAL-7B FOR RTX 5050 (8GB VRAM)\", flush=True)\n",
    "print(\"=\" * 70, flush=True)\n",
    "\n",
    "model_name = RAG_CONFIG['generation']['model_name']\n",
    "model_loaded = False\n",
    "mistral_model = None\n",
    "mistral_tokenizer = None\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\", flush=True)\n",
    "    print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\", flush=True)\n",
    "    print(f\"Initial GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\", flush=True)\n",
    "\n",
    "try:\n",
    "    # Step 1: Load Tokenizer\n",
    "    print(f\"\\nüìù Loading tokenizer for {model_name}...\", flush=True)\n",
    "    mistral_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if mistral_tokenizer.pad_token is None:\n",
    "        mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "    print(\"‚úÖ Tokenizer loaded successfully\", flush=True)\n",
    "    \n",
    "    # Step 2: Configure 4-bit Quantization (Essential for 8GB VRAM)\n",
    "    print(\"\\n‚öôÔ∏è Configuring 4-bit quantization for 8GB VRAM...\", flush=True)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                      # Use 4-bit quantization\n",
    "        bnb_4bit_compute_dtype=torch.float16,   # Compute in float16\n",
    "        bnb_4bit_use_double_quant=True,         # Double quantization for extra memory savings\n",
    "        bnb_4bit_quant_type=\"nf4\"               # NormalFloat 4-bit quantization\n",
    "    )\n",
    "    print(\"‚úÖ Quantization config ready (4-bit NF4 + double quant)\", flush=True)\n",
    "    \n",
    "    # Step 3: Load Model with Quantization\n",
    "    print(f\"\\nüîÑ Loading {model_name} with 4-bit quantization...\", flush=True)\n",
    "    print(\"üì¶ This will download ~13GB if not cached (may take 5-10 minutes)\", flush=True)\n",
    "    print(\"‚è≥ Please wait...\", flush=True)\n",
    "    \n",
    "    mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",                      # Automatically distribute across GPU\n",
    "        torch_dtype=torch.float16,              # Use float16 for memory efficiency\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True                  # Minimize CPU memory during loading\n",
    "    )\n",
    "    \n",
    "    # Clear cache after loading\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    model_loaded = True\n",
    "    \n",
    "    # Display Success and Memory Stats\n",
    "    print(\"\\n\" + \"=\" * 70, flush=True)\n",
    "    print(\"‚úÖ MODEL LOADED SUCCESSFULLY!\", flush=True)\n",
    "    print(\"=\" * 70, flush=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "        memory_free = 8 - memory_reserved\n",
    "        \n",
    "        print(f\"üíæ GPU Memory Status:\", flush=True)\n",
    "        print(f\"   Allocated:  {memory_allocated:.2f} GB\", flush=True)\n",
    "        print(f\"   Reserved:   {memory_reserved:.2f} GB\", flush=True)\n",
    "        print(f\"   Free:       {memory_free:.2f} GB\", flush=True)\n",
    "        print(f\"   Total:      8.00 GB\", flush=True)\n",
    "        print(f\"   Usage:      {(memory_reserved/8)*100:.1f}%\", flush=True)\n",
    "        \n",
    "        # Memory status indicator\n",
    "        if memory_reserved < 6.4:  # < 80%\n",
    "            print(f\"   Status:     üü¢ Excellent - Plenty of headroom\", flush=True)\n",
    "        elif memory_reserved < 7.2:  # < 90%\n",
    "            print(f\"   Status:     üü° Good - Monitor memory usage\", flush=True)\n",
    "        else:\n",
    "            print(f\"   Status:     üî¥ Tight - Be cautious with batch sizes\", flush=True)\n",
    "    \n",
    "    print(\"\\nüéØ Optimizations Applied for 8GB VRAM:\", flush=True)\n",
    "    print(\"   ‚úÖ 4-bit NF4 quantization (~4GB model size)\", flush=True)\n",
    "    print(\"   ‚úÖ Double quantization for extra savings\", flush=True)\n",
    "    print(\"   ‚úÖ Float16 compute dtype\", flush=True)\n",
    "    print(\"   ‚úÖ Automatic device mapping\", flush=True)\n",
    "    print(\"   ‚úÖ Low CPU memory usage mode\", flush=True)\n",
    "    \n",
    "    print(\"\\nüìä Model Configuration:\", flush=True)\n",
    "    print(f\"   Model:       {model_name}\", flush=True)\n",
    "    print(f\"   Precision:   4-bit (quantized from 16-bit)\", flush=True)\n",
    "    print(f\"   Device:      {next(mistral_model.parameters()).device}\", flush=True)\n",
    "    print(f\"   Parameters:  ~7B (quantized)\", flush=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\" * 70, flush=True)\n",
    "    print(\"‚ùå MODEL LOADING FAILED\", flush=True)\n",
    "    print(\"=\" * 70, flush=True)\n",
    "    print(f\"Error: {str(e)}\", flush=True)\n",
    "    print(\"\\nüí° Troubleshooting Steps:\", flush=True)\n",
    "    print(\"1. ‚úÖ Check that you're logged in to Hugging Face (run previous cell)\", flush=True)\n",
    "    print(\"2. ‚úÖ Ensure you have internet connection for download\", flush=True)\n",
    "    print(\"3. ‚úÖ Verify you have ~15GB free disk space\", flush=True)\n",
    "    print(\"4. ‚úÖ Check that CUDA is available: torch.cuda.is_available()\", flush=True)\n",
    "    print(\"5. ‚úÖ Try restarting the notebook kernel\", flush=True)\n",
    "    print(\"\\nüí° If issues persist:\", flush=True)\n",
    "    print(\"   - Install/update: pip install -U transformers accelerate bitsandbytes\", flush=True)\n",
    "    print(\"   - Clear HF cache: rm -rf ~/.cache/huggingface/\", flush=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    model_loaded = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70, flush=True)\n",
    "print(f\"üèÅ Final Status: {'‚úÖ READY TO USE' if model_loaded else '‚ùå NOT LOADED'}\", flush=True)\n",
    "print(\"=\" * 70, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef23ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralRAGGenerator:\n",
    "    \"\"\"Memory-optimized RAG system using Mistral-7B-Instruct for RTX 5050 (8GB VRAM)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, retriever, config):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.retriever = retriever\n",
    "        self.config = config\n",
    "\n",
    "        # üîß Optimized system prompt (uses second version's [INST] style)\n",
    "        self.system_prompt = \"\"\"<s>[INST] You are a university student advisor with access to the official student handbook.\n",
    "Your task is to answer the student's question accurately using only the provided handbook context.\n",
    "\n",
    "Guidelines:\n",
    "- Use only the provided context to answer.\n",
    "- If the context doesn‚Äôt include enough information, say so clearly.\n",
    "- Be specific about policies, procedures, and requirements.\n",
    "- Keep your response concise and factual.\n",
    "\n",
    "HANDBOOK CONTEXT:\n",
    "{context}\n",
    "\n",
    "STUDENT QUESTION:\n",
    "{question}\n",
    "\n",
    "Provide a clear, helpful answer based on the handbook context above. \n",
    "If the handbook lacks enough information, explicitly say so. [/INST]\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    def format_context(self, retrieved_chunks):\n",
    "        \"\"\"Format retrieved chunks into context string\"\"\"\n",
    "        if not retrieved_chunks:\n",
    "            return \"No relevant information found.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "            context_parts.append(f\"[Section {i}] {chunk['text']}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_response(self, question, max_new_tokens=200, temperature=None, top_p=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Generate RAG response optimized for 8GB VRAM GPUs.\n",
    "        - Uses reduced context and token limits\n",
    "        - Clears GPU memory between steps\n",
    "        - Enables KV cache for faster generation\n",
    "        \"\"\"\n",
    "        temperature = temperature or self.config['generation']['temperature']\n",
    "        top_p = top_p or self.config['generation']['top_p']\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n‚ùì Question: {question}\")\n",
    "            print(\"=\" * 70)\n",
    "        \n",
    "        # üßπ Clear GPU cache before generation\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # üîç Step 1: Retrieve relevant context\n",
    "        if verbose:\n",
    "            print(\"üîç Retrieving relevant context...\")\n",
    "        retrieved_chunks = self.retriever.retrieve(question)\n",
    "        \n",
    "        if not retrieved_chunks:\n",
    "            if verbose:\n",
    "                print(\"‚ö†Ô∏è No relevant context found.\")\n",
    "            return {\n",
    "                'question': question,\n",
    "                'response': \"I couldn‚Äôt find relevant information in the student handbook to answer your question.\",\n",
    "                'retrieved_chunks': [],\n",
    "                'context_used': \"\"\n",
    "            }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"üìö Found {len(retrieved_chunks)} relevant chunks:\")\n",
    "            for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "                print(f\"   {i}. Relevance score: {chunk['score']:.4f}\")\n",
    "        \n",
    "        # üß† Step 2: Format context (limit length for VRAM efficiency)\n",
    "        context = self.format_context(retrieved_chunks)\n",
    "        context_limited = context[:1200]  # 1.2k chars keeps VRAM below 8GB\n",
    "        \n",
    "        # üß© Step 3: Construct optimized prompt\n",
    "        prompt = self.system_prompt.format(context=context_limited, question=question)\n",
    "        \n",
    "        # ‚úÇÔ∏è Step 4: Tokenize input efficiently\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024  # Lower to prevent OOM on 8GB GPUs\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # üìä GPU memory check before generation\n",
    "        if verbose and torch.cuda.is_available():\n",
    "            mem_before = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"üíæ GPU memory before generation: {mem_before:.2f} GB\")\n",
    "        \n",
    "        # ‚öôÔ∏è Step 5: Generate response (no grad, cache on)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,      # keep small for VRAM\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                do_sample=self.config['generation']['do_sample'],\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                use_cache=True                       # enables faster generation\n",
    "            )\n",
    "        \n",
    "        # üßæ Step 6: Decode and extract answer\n",
    "        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        if \"Answer:\" in full_output:\n",
    "            response = full_output.split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            response = full_output[len(prompt):].strip()\n",
    "        \n",
    "        # üßπ Step 7: Cleanup GPU memory\n",
    "        del inputs, outputs\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            if verbose:\n",
    "                mem_after = torch.cuda.memory_allocated() / 1e9\n",
    "                print(f\"üíæ GPU memory after cleanup: {mem_after:.2f} GB\")\n",
    "        \n",
    "        # ü™Ñ Step 8: Display and return\n",
    "        if verbose:\n",
    "            print(\"\\nüí° Answer:\")\n",
    "            print(\"-\" * 70)\n",
    "            print(response)\n",
    "            print(\"-\" * 70)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'response': response,\n",
    "            'retrieved_chunks': retrieved_chunks,\n",
    "            'context_used': context_limited\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1765af69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîß INITIALIZING RAG SYSTEM\n",
      "======================================================================\n",
      "‚úÖ RAG system initialized successfully!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG system\n",
    "if model_loaded and mistral_model is not None and mistral_tokenizer is not None:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîß INITIALIZING RAG SYSTEM\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    rag_system = MistralRAGGenerator(\n",
    "        model=mistral_model,\n",
    "        tokenizer=mistral_tokenizer,\n",
    "        retriever=retriever,\n",
    "        config=RAG_CONFIG\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ RAG system initialized successfully!\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize RAG system - model or tokenizer not loaded\")\n",
    "    rag_system = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4ccf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG system ready for testing!\n"
     ]
    }
   ],
   "source": [
    "# Test the RAG system with sample questions\n",
    "test_questions = [\n",
    "    \"What are the admission requirements?\",\n",
    "    \"What is the grading system?\",\n",
    "    \"How do I apply for financial aid?\",\n",
    "    \"What are the library hours?\",\n",
    "    \"What is the academic calendar?\"\n",
    "]\n",
    "\n",
    "def test_rag_system(questions=None):\n",
    "    \"\"\"Test RAG system with multiple questions\"\"\"\n",
    "    if not model_loaded:\n",
    "        print(\"Cannot test - model not loaded\")\n",
    "        return\n",
    "    \n",
    "    questions = questions or test_questions\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Testing RAG system with {len(questions)} questions...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\nTEST {i}/{len(questions)}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = rag_system.generate_response(question)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        result['elapsed_time'] = elapsed_time\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"\\nTime taken: {elapsed_time:.2f} seconds\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run quick test (uncomment to test)\n",
    "# test_results = test_rag_system(test_questions[:2])  # Test first 2 questions\n",
    "\n",
    "print(\"RAG system ready for testing!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0344069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Enhanced interactive chat function ready!\n",
      "======================================================================\n",
      "ü§ñ INTERACTIVE RAG CHAT - RTX 5050 OPTIMIZED\n",
      "======================================================================\n",
      "Ask questions about the student handbook!\n",
      "\n",
      "üìã Commands:\n",
      "  'quit' or 'exit' - End chat\n",
      "  'help' - Show commands\n",
      "  'memory' - Show GPU memory usage\n",
      "  'history' - Show chat history\n",
      "  'clear' - Clear chat history\n",
      "  'save' - Save chat history to file\n",
      "======================================================================\n",
      "\n",
      "üü¢ GPU Memory: 4.6GB (57%)\n",
      "\n",
      "‚ùì Question: What Hair Color Can Boys Wear?\n",
      "======================================================================\n",
      "üîç Retrieving relevant context...\n",
      "üìö Found 5 relevant chunks:\n",
      "   1. Relevance score: 0.5730\n",
      "   2. Relevance score: 0.5679\n",
      "   3. Relevance score: 0.4925\n",
      "   4. Relevance score: 0.4840\n",
      "   5. Relevance score: 0.4799\n",
      "üíæ GPU memory before generation: 4.58 GB\n",
      "üíæ GPU memory after cleanup: 4.58 GB\n",
      "\n",
      "üí° Answer:\n",
      "----------------------------------------------------------------------\n",
      "Boys can wear any hair color as long as they do not have highlights. Working students with job-required hairstyles must secure a special permit from OSAS.\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚è±Ô∏è Response time: 2.2s\n",
      "\n",
      "üü¢ GPU Memory: 4.6GB (57%)\n",
      "\n",
      "‚ùì Question: What are the disciplinary actions for students who violate school rules?\n",
      "======================================================================\n",
      "üîç Retrieving relevant context...\n",
      "üìö Found 5 relevant chunks:\n",
      "   1. Relevance score: 0.6766\n",
      "   2. Relevance score: 0.6669\n",
      "   3. Relevance score: 0.6650\n",
      "   4. Relevance score: 0.6396\n",
      "   5. Relevance score: 0.6346\n",
      "üíæ GPU memory before generation: 4.58 GB\n",
      "üíæ GPU memory after cleanup: 4.58 GB\n",
      "\n",
      "üí° Answer:\n",
      "----------------------------------------------------------------------\n",
      "According to the provided handbook context, disciplinary actions for students who violate school rules can vary depending on the type of offense committed. For example, if a student engages in disrespect or discourtesy towards a faculty member or school personnel, they may face disciplinary action as outlined in Section 1, specifically in Article 4.1.1.6. Similarly, if a student commits vandalism, theft, robbery, arson, or destruction of school property, they may face disciplinary action as outlined in Section 3, specifically in Article 4.1.1.16. If a student engages in lewd, indecent, or obscene conduct within the school premises, they may face disciplinary action as outlined in Section 3, specifically in Article 4.1.1.18. If a student commits littering within the school premises, they may face disciplinary action as outlined in Section 5, specifically in Article 4.1.2.5.\n",
      "\n",
      "It's important to note that the handbook context does not provide a comprehensive list of disciplinary actions for all types of offenses, and the specific actions taken may depend on the severity\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "‚è±Ô∏è Response time: 14.2s\n",
      "\n",
      "üü¢ GPU Memory: 4.6GB (57%)\n",
      "üëã Chat ended. Goodbye!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'What Hair Color Can Boys Wear?',\n",
       "  'response': 'Boys can wear any hair color as long as they do not have highlights. Working students with job-required hairstyles must secure a special permit from OSAS.',\n",
       "  'retrieved_chunks': [{'text': \"Male students must wear a clean haircut (barber's cut 2‚Äù x 3‚Äù) without highlights. Working students with job-required hairstyles must secure a special permit from OSAS.\",\n",
       "    'score': 0.5729842782020569,\n",
       "    'chunk_id': 226},\n",
       "   {'text': 'Female students are not allowed to have colored highlights in their hair.',\n",
       "    'score': 0.5679453611373901,\n",
       "    'chunk_id': 228},\n",
       "   {'text': 'Junior ‚Äì third year student.',\n",
       "    'score': 0.4925153851509094,\n",
       "    'chunk_id': 581},\n",
       "   {'text': 'Senior ‚Äì fourth year student.',\n",
       "    'score': 0.48395243287086487,\n",
       "    'chunk_id': 598},\n",
       "   {'text': 'Proper Grooming\\n      * 3.3.1.',\n",
       "    'score': 0.479887455701828,\n",
       "    'chunk_id': 225}],\n",
       "  'context_used': \"[Section 1] Male students must wear a clean haircut (barber's cut 2‚Äù x 3‚Äù) without highlights. Working students with job-required hairstyles must secure a special permit from OSAS.\\n\\n[Section 2] Female students are not allowed to have colored highlights in their hair.\\n\\n[Section 3] Junior ‚Äì third year student.\\n\\n[Section 4] Senior ‚Äì fourth year student.\\n\\n[Section 5] Proper Grooming\\n      * 3.3.1.\",\n",
       "  'timestamp': '2025-10-20 00:33:20',\n",
       "  'response_time': 2.194418430328369},\n",
       " {'question': 'What are the disciplinary actions for students who violate school rules?',\n",
       "  'response': \"According to the provided handbook context, disciplinary actions for students who violate school rules can vary depending on the type of offense committed. For example, if a student engages in disrespect or discourtesy towards a faculty member or school personnel, they may face disciplinary action as outlined in Section 1, specifically in Article 4.1.1.6. Similarly, if a student commits vandalism, theft, robbery, arson, or destruction of school property, they may face disciplinary action as outlined in Section 3, specifically in Article 4.1.1.16. If a student engages in lewd, indecent, or obscene conduct within the school premises, they may face disciplinary action as outlined in Section 3, specifically in Article 4.1.1.18. If a student commits littering within the school premises, they may face disciplinary action as outlined in Section 5, specifically in Article 4.1.2.5.\\n\\nIt's important to note that the handbook context does not provide a comprehensive list of disciplinary actions for all types of offenses, and the specific actions taken may depend on the severity\",\n",
       "  'retrieved_chunks': [{'text': 'Disrespect or discourtesy to any faculty member or school personnel\\n\\n            * 4.1.1.6.',\n",
       "    'score': 0.676616907119751,\n",
       "    'chunk_id': 243},\n",
       "   {'text': 'Other violations of school rules and regulations as may be determined by the concerned authorities\\nARTICLE IX\\n STUDENT AFFAIRS AND SERVICES (SAS)\\n (CMO NO.09, Series of 2013 and CMO No.',\n",
       "    'score': 0.6669473052024841,\n",
       "    'chunk_id': 275},\n",
       "   {'text': 'Preventing faculty members and school employees from performing their duties or from entering the school premises\\n\\n            * 4.1.1.16. Vandalism, theft, robbery, arson and destruction of school property\\n\\n            * 4.1.1.17. Instigating or participating in any student demonstration, rally, picketing, and other forms of mass action without prior clearance from the Office of the President\\n\\n            * 4.1.1.18. Engaging in lewd, indecent or obscene conduct within the school premises\\n\\n            * 4.1.1.19.',\n",
       "    'score': 0.6649558544158936,\n",
       "    'chunk_id': 251},\n",
       "   {'text': 'STUDENT DISCIPLINE\\n4.1. Types of Offenses\\n4.1.1.',\n",
       "    'score': 0.6396291851997375,\n",
       "    'chunk_id': 238},\n",
       "   {'text': 'Littering within the school premises\\n\\n               * 4.1.2.5.',\n",
       "    'score': 0.6346448659896851,\n",
       "    'chunk_id': 265}],\n",
       "  'context_used': '[Section 1] Disrespect or discourtesy to any faculty member or school personnel\\n\\n            * 4.1.1.6.\\n\\n[Section 2] Other violations of school rules and regulations as may be determined by the concerned authorities\\nARTICLE IX\\n STUDENT AFFAIRS AND SERVICES (SAS)\\n (CMO NO.09, Series of 2013 and CMO No.\\n\\n[Section 3] Preventing faculty members and school employees from performing their duties or from entering the school premises\\n\\n            * 4.1.1.16. Vandalism, theft, robbery, arson and destruction of school property\\n\\n            * 4.1.1.17. Instigating or participating in any student demonstration, rally, picketing, and other forms of mass action without prior clearance from the Office of the President\\n\\n            * 4.1.1.18. Engaging in lewd, indecent or obscene conduct within the school premises\\n\\n            * 4.1.1.19.\\n\\n[Section 4] STUDENT DISCIPLINE\\n4.1. Types of Offenses\\n4.1.1.\\n\\n[Section 5] Littering within the school premises\\n\\n               * 4.1.2.5.',\n",
       "  'timestamp': '2025-10-20 00:33:56',\n",
       "  'response_time': 14.20621371269226}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def interactive_rag_chat():\n",
    "    \"\"\"Enhanced interactive chat interface optimized for RTX 5050\"\"\"\n",
    "    if not model_loaded:\n",
    "        print(\"‚ùå Cannot start chat - model not loaded\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ü§ñ INTERACTIVE RAG CHAT - RTX 5050 OPTIMIZED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Ask questions about the student handbook!\")\n",
    "    print(\"\\nüìã Commands:\")\n",
    "    print(\"  'quit' or 'exit' - End chat\")\n",
    "    print(\"  'help' - Show commands\")\n",
    "    print(\"  'memory' - Show GPU memory usage\")\n",
    "    print(\"  'history' - Show chat history\")\n",
    "    print(\"  'clear' - Clear chat history\")\n",
    "    print(\"  'save' - Save chat history to file\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    chat_history = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Show memory status\n",
    "            if torch.cuda.is_available():\n",
    "                memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "                memory_percent = (memory_used / 8) * 100\n",
    "                status = \"üü¢\" if memory_percent < 80 else \"üü°\" if memory_percent < 90 else \"üî¥\"\n",
    "                print(f\"\\n{status} GPU Memory: {memory_used:.1f}GB ({memory_percent:.0f}%)\")\n",
    "            \n",
    "            user_input = input(\"\\nüí¨ Your question: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"üëã Chat ended. Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            elif user_input.lower() == 'help':\n",
    "                print(\"\\nüìã Available Commands:\")\n",
    "                print(\"  help - Show this help\")\n",
    "                print(\"  quit/exit/q - End chat\")\n",
    "                print(\"  memory - Show detailed GPU memory info\")\n",
    "                print(\"  history - Show chat history\")\n",
    "                print(\"  clear - Clear chat history\")\n",
    "                print(\"  save - Save chat history to file\")\n",
    "                print(\"  settings - Show current RAG settings\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'memory':\n",
    "                if torch.cuda.is_available():\n",
    "                    memory_allocated = torch.cuda.memory_allocated() / 1e9\n",
    "                    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "                    memory_free = 8 - memory_reserved\n",
    "                    print(f\"\\nüîç GPU Memory Status:\")\n",
    "                    print(f\"  Allocated: {memory_allocated:.2f} GB\")\n",
    "                    print(f\"  Reserved:  {memory_reserved:.2f} GB\")\n",
    "                    print(f\"  Free:      {memory_free:.2f} GB\")\n",
    "                    print(f\"  Total:     8.0 GB\")\n",
    "                else:\n",
    "                    print(\"‚ùå CUDA not available\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'settings':\n",
    "                print(f\"\\n‚öôÔ∏è Current RAG Settings:\")\n",
    "                print(f\"  Temperature: {RAG_CONFIG['generation']['temperature']}\")\n",
    "                print(f\"  Top-p: {RAG_CONFIG['generation']['top_p']}\")\n",
    "                print(f\"  Max new tokens: 256 (optimized for RTX 5050)\")\n",
    "                print(f\"  Retrieval top-k: {RAG_CONFIG['retrieval']['top_k']}\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'history':\n",
    "                if chat_history:\n",
    "                    print(f\"\\nüìú Chat History ({len(chat_history)} questions):\")\n",
    "                    for i, item in enumerate(chat_history, 1):\n",
    "                        print(f\"  {i}. {item['question'][:60]}{'...' if len(item['question']) > 60 else ''}\")\n",
    "                else:\n",
    "                    print(\"\\nüìú No chat history yet.\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'clear':\n",
    "                chat_history.clear()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                print(\"üóëÔ∏è Chat history cleared and GPU cache cleaned.\")\n",
    "                continue\n",
    "            \n",
    "            elif user_input.lower() == 'save':\n",
    "                if chat_history:\n",
    "                    filename = f\"chat_history_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "                    with open(filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump([{\n",
    "                            'question': item['question'],\n",
    "                            'response': item['response'],\n",
    "                            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        } for item in chat_history], f, indent=2)\n",
    "                    print(f\"üíæ Chat history saved to {filename}\")\n",
    "                else:\n",
    "                    print(\"üìú No chat history to save.\")\n",
    "                continue\n",
    "            \n",
    "            elif not user_input:\n",
    "                print(\"‚ùì Please enter a question.\")\n",
    "                continue\n",
    "            \n",
    "            # Generate response with timing\n",
    "            start_time = time.time()\n",
    "            result = rag_system.generate_response(user_input, max_new_tokens=256, verbose=True)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            result['timestamp'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            result['response_time'] = elapsed_time\n",
    "            chat_history.append(result)\n",
    "            \n",
    "            print(f\"\\n‚è±Ô∏è Response time: {elapsed_time:.1f}s\")\n",
    "            \n",
    "            # Memory cleanup after each response\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n‚ö†Ô∏è Chat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    return chat_history\n",
    "\n",
    "# Start interactive chat (uncomment to use)\n",
    "# chat_history = interactive_rag_chat()\n",
    "\n",
    "print(\"üöÄ Enhanced interactive chat function ready!\")\n",
    "\n",
    "interactive_rag_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff08040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process_questions(questions, output_file=None, memory_cleanup_interval=5):\n",
    "    \"\"\"\n",
    "    Batch process multiple questions with memory optimization for RTX 5050\n",
    "    \n",
    "    Args:\n",
    "        questions: List of questions (strings) or list of dicts with 'question' key\n",
    "        output_file: Optional filename to save results\n",
    "        memory_cleanup_interval: Clean GPU memory every N questions\n",
    "    \"\"\"\n",
    "    if not model_loaded:\n",
    "        print(\"‚ùå Cannot process - model not loaded\")\n",
    "        return None\n",
    "    \n",
    "    # Handle different input formats\n",
    "    if isinstance(questions, str):\n",
    "        questions = [questions]\n",
    "    \n",
    "    question_list = []\n",
    "    for q in questions:\n",
    "        if isinstance(q, str):\n",
    "            question_list.append(q)\n",
    "        elif isinstance(q, dict) and 'question' in q:\n",
    "            question_list.append(q['question'])\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping invalid question format: {q}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîÑ BATCH PROCESSING MODE - RTX 5050 OPTIMIZED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìä Processing {len(question_list)} questions\")\n",
    "    print(f\"üßπ Memory cleanup every {memory_cleanup_interval} questions\")\n",
    "    print(f\"üíæ Output file: {output_file or 'None (in-memory only)'}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, question in enumerate(question_list, 1):\n",
    "        print(f\"\\nüìù Processing {i}/{len(question_list)}: {question[:60]}{'...' if len(question) > 60 else ''}\")\n",
    "        \n",
    "        # Memory status before processing\n",
    "        if torch.cuda.is_available():\n",
    "            memory_before = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"   üíæ GPU Memory: {memory_before:.1f}GB\")\n",
    "        \n",
    "        try:\n",
    "            # Generate response with reduced verbosity for batch mode\n",
    "            question_start = time.time()\n",
    "            result = rag_system.generate_response(\n",
    "                question, \n",
    "                max_new_tokens=256, \n",
    "                verbose=False  # Reduce output for batch processing\n",
    "            )\n",
    "            question_time = time.time() - question_start\n",
    "            \n",
    "            # Add metadata\n",
    "            result.update({\n",
    "                'batch_index': i,\n",
    "                'processing_time': question_time,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'gpu_memory_before': memory_before if torch.cuda.is_available() else None\n",
    "            })\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"   ‚úÖ Completed in {question_time:.1f}s\")\n",
    "            print(f\"   üìã Found {len(result['retrieved_chunks'])} relevant chunks\")\n",
    "            \n",
    "            # Memory cleanup at intervals\n",
    "            if i % memory_cleanup_interval == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                    memory_after = torch.cuda.memory_allocated() / 1e9\n",
    "                    print(f\"   üßπ Memory cleaned: {memory_after:.1f}GB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing question {i}: {e}\")\n",
    "            error_result = {\n",
    "                'question': question,\n",
    "                'response': f\"Error: {str(e)}\",\n",
    "                'retrieved_chunks': [],\n",
    "                'context_used': \"\",\n",
    "                'batch_index': i,\n",
    "                'processing_time': 0,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'error': str(e)\n",
    "            }\n",
    "            results.append(error_result)\n",
    "            \n",
    "            # Force memory cleanup on error\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Final cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    successful_results = [r for r in results if 'error' not in r]\n",
    "    error_count = len(results) - len(successful_results)\n",
    "    avg_time = np.mean([r['processing_time'] for r in successful_results]) if successful_results else 0\n",
    "    total_chunks = sum(len(r['retrieved_chunks']) for r in successful_results)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä BATCH PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"‚úÖ Successful: {len(successful_results)}/{len(question_list)}\")\n",
    "    print(f\"‚ùå Errors: {error_count}\")\n",
    "    print(f\"‚è±Ô∏è Total time: {total_time:.1f}s\")\n",
    "    print(f\"‚ö° Average time per question: {avg_time:.1f}s\")\n",
    "    print(f\"üìã Total chunks retrieved: {total_chunks}\")\n",
    "    print(f\"üß† Average chunks per question: {total_chunks/len(successful_results) if successful_results else 0:.1f}\")\n",
    "    \n",
    "    # Save results if requested\n",
    "    if output_file:\n",
    "        try:\n",
    "            # Create serializable version\n",
    "            serializable_results = []\n",
    "            for result in results:\n",
    "                serialized = {\n",
    "                    'question': result['question'],\n",
    "                    'response': result['response'],\n",
    "                    'batch_index': result['batch_index'],\n",
    "                    'processing_time': result['processing_time'],\n",
    "                    'timestamp': result['timestamp'],\n",
    "                    'num_chunks_retrieved': len(result['retrieved_chunks']),\n",
    "                    'retrieval_scores': [chunk['score'] for chunk in result['retrieved_chunks']],\n",
    "                    'error': result.get('error', None)\n",
    "                }\n",
    "                serializable_results.append(serialized)\n",
    "            \n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'metadata': {\n",
    "                        'total_questions': len(question_list),\n",
    "                        'successful_results': len(successful_results),\n",
    "                        'error_count': error_count,\n",
    "                        'total_processing_time': total_time,\n",
    "                        'average_time_per_question': avg_time,\n",
    "                        'processing_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'model_config': RAG_CONFIG\n",
    "                    },\n",
    "                    'results': serializable_results\n",
    "                }, f, indent=2)\n",
    "            \n",
    "            print(f\"üíæ Results saved to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving results: {e}\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    return results\n",
    "\n",
    "def load_questions_from_file(filepath):\n",
    "    \"\"\"Load questions from various file formats\"\"\"\n",
    "    try:\n",
    "        if filepath.endswith('.json'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    return data\n",
    "                elif isinstance(data, dict):\n",
    "                    # Handle Questions.json format\n",
    "                    questions = []\n",
    "                    for category, q_list in data.items():\n",
    "                        questions.extend(q_list)\n",
    "                    return questions\n",
    "        \n",
    "        elif filepath.endswith('.txt'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                return [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚ùå Unsupported file format: {filepath}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading questions from {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage functions\n",
    "def batch_process_test_questions():\n",
    "    \"\"\"Process the predefined test questions\"\"\"\n",
    "    test_questions = [\n",
    "        \"What are the admission requirements?\",\n",
    "        \"What is the grading system?\",\n",
    "        \"How do I apply for financial aid?\",\n",
    "        \"What are the library hours?\",\n",
    "        \"What is the academic calendar?\",\n",
    "        \"What are the graduation requirements?\",\n",
    "        \"How do I register for classes?\",\n",
    "        \"What academic support services are available?\"\n",
    "    ]\n",
    "    \n",
    "    return batch_process_questions(\n",
    "        test_questions, \n",
    "        output_file=f\"batch_test_results_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    )\n",
    "\n",
    "def batch_process_questions_json():\n",
    "    \"\"\"Process all questions from Questions.json file\"\"\"\n",
    "    questions = load_questions_from_file('../Retriever/Questions.json')\n",
    "    if questions:\n",
    "        return batch_process_questions(\n",
    "            questions, \n",
    "            output_file=f\"batch_all_questions_{time.strftime('%Y%m%d_%H%M%S')}.json\",\n",
    "            memory_cleanup_interval=3  # More frequent cleanup for larger batches\n",
    "        )\n",
    "    else:\n",
    "        print(\"‚ùå Could not load questions from Questions.json\")\n",
    "        return None\n",
    "\n",
    "print(\"üîÑ Advanced batch processing functions ready!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"  batch_process_test_questions()  # Process 8 test questions\")\n",
    "print(\"  batch_process_questions_json()  # Process all Questions.json\")\n",
    "print(\"  batch_process_questions(['Q1', 'Q2'], 'results.json')  # Custom questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290c3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_with_questions_json():\n",
    "    \"\"\"Evaluate RAG system using your Questions.json file\"\"\"\n",
    "    if not model_loaded:\n",
    "        print(\"Cannot evaluate - model not loaded\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        with open('../Retriever/Questions.json', 'r', encoding='utf-8') as f:\n",
    "            questions_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Questions.json not found. Make sure it exists in ../Retriever/\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"RAG SYSTEM EVALUATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    all_results = []\n",
    "    total_questions = sum(len(questions) for questions in questions_data.values())\n",
    "    current_q = 0\n",
    "    \n",
    "    for category, questions in questions_data.items():\n",
    "        print(f\"\\nEvaluating Category: {category}\")\n",
    "        print(f\"Questions: {len(questions)}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        category_results = []\n",
    "        \n",
    "        for q_item in questions:\n",
    "            current_q += 1\n",
    "            question = q_item['question']\n",
    "            expected_ref = q_item['expected_reference']\n",
    "            \n",
    "            print(f\"\\n[{current_q}/{total_questions}] {question}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = rag_system.generate_response(question)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            result['expected_reference'] = expected_ref\n",
    "            result['category'] = category\n",
    "            result['elapsed_time'] = elapsed_time\n",
    "            \n",
    "            category_results.append(result)\n",
    "            all_results.append(result)\n",
    "            \n",
    "            print(f\"Time: {elapsed_time:.2f}s\")\n",
    "            print(\"-\" * 30)\n",
    "        \n",
    "        # Category summary\n",
    "        avg_time = np.mean([r['elapsed_time'] for r in category_results])\n",
    "        avg_chunks = np.mean([len(r['retrieved_chunks']) for r in category_results])\n",
    "        print(f\"\\nCategory Summary:\")\n",
    "        print(f\"  Average response time: {avg_time:.2f}s\")\n",
    "        print(f\"  Average chunks retrieved: {avg_chunks:.1f}\")\n",
    "    \n",
    "    # Overall summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATION COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total questions: {len(all_results)}\")\n",
    "    print(f\"Average response time: {np.mean([r['elapsed_time'] for r in all_results]):.2f}s\")\n",
    "    print(f\"Total evaluation time: {sum([r['elapsed_time'] for r in all_results]):.1f}s\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def save_evaluation_results(results, filename=\"rag_evaluation_results.json\"):\n",
    "    \"\"\"Save evaluation results to file\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to save\")\n",
    "        return\n",
    "    \n",
    "    # Convert to serializable format\n",
    "    serializable_results = []\n",
    "    for result in results:\n",
    "        serialized = {\n",
    "            'question': result['question'],\n",
    "            'response': result['response'],\n",
    "            'expected_reference': result.get('expected_reference', ''),\n",
    "            'category': result.get('category', ''),\n",
    "            'elapsed_time': result['elapsed_time'],\n",
    "            'num_chunks_retrieved': len(result['retrieved_chunks']),\n",
    "            'retrieval_scores': [chunk['score'] for chunk in result['retrieved_chunks']]\n",
    "        }\n",
    "        serializable_results.append(serialized)\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Evaluation results saved to {filename}\")\n",
    "\n",
    "# Run evaluation (uncomment to evaluate)\n",
    "# evaluation_results = evaluate_rag_with_questions_json()\n",
    "# save_evaluation_results(evaluation_results)\n",
    "\n",
    "print(\"Evaluation functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ QUICK START - Test Your RAG System on RTX 5050\n",
    "\n",
    "if model_loaded and rag_system is not None:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üöÄ TESTING RAG SYSTEM ON RTX 5050 (8GB VRAM)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Show current GPU memory status\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "        memory_percent = (memory_used / 8) * 100\n",
    "        \n",
    "        if memory_percent < 80:\n",
    "            status = \"\udfe2 Excellent\"\n",
    "        elif memory_percent < 90:\n",
    "            status = \"üü° Good\"\n",
    "        else:\n",
    "            status = \"üî¥ High\"\n",
    "        \n",
    "        print(f\"\\nüíæ GPU Memory: {memory_used:.1f} GB / 8 GB ({memory_percent:.0f}%) - {status}\")\n",
    "    \n",
    "    # Test with a sample question\n",
    "    sample_question = \"What are the admission requirements?\"\n",
    "    \n",
    "    print(f\"\\nüß™ Testing with sample question:\")\n",
    "    print(f\"'{sample_question}'\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Generate response with timing\n",
    "    start_time = time.time()\n",
    "    result = rag_system.generate_response(sample_question, max_new_tokens=200, verbose=True)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è Response generated in {elapsed_time:.1f} seconds\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚úÖ SUCCESS! Your RAG system is working on RTX 5050!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Final memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        final_memory = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"üßπ Memory after cleanup: {final_memory:.1f} GB / 8 GB\")\n",
    "    \n",
    "    # Usage guide\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\udcd6 HOW TO USE YOUR RAG SYSTEM\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\n1Ô∏è‚É£  Ask a Single Question:\")\n",
    "    print(\"   result = rag_system.generate_response('your question here')\")\n",
    "    \n",
    "    print(\"\\n2Ô∏è‚É£  Interactive Chat Mode:\")\n",
    "    print(\"   interactive_rag_chat()\")\n",
    "    \n",
    "    print(\"\\n3Ô∏è‚É£  Batch Process Multiple Questions:\")\n",
    "    print(\"   batch_process_test_questions()\")\n",
    "    \n",
    "    print(\"\\n4Ô∏è‚É£  Check GPU Memory:\")\n",
    "    print(\"   torch.cuda.memory_allocated() / 1e9  # Shows GB used\")\n",
    "    print(\"   torch.cuda.empty_cache()             # Frees unused memory\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"\udca1 TIPS FOR 8GB VRAM:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚úÖ Keep max_new_tokens ‚â§ 250 for safety\")\n",
    "    print(\"‚úÖ Process questions one at a time or use small batches\")\n",
    "    print(\"‚úÖ Run torch.cuda.empty_cache() if you get OOM errors\")\n",
    "    print(\"‚úÖ Monitor memory with the 'memory' command in interactive mode\")\n",
    "    print(\"‚úÖ Close other GPU-using applications for best performance\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ YOU'RE ALL SET! Start asking questions about the student handbook!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "else:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"‚ùå RAG SYSTEM NOT READY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nPlease run all cells in order:\")\n",
    "    print(\"1. ‚úÖ Cell 1: Import libraries and check GPU\")\n",
    "    print(\"2. ‚úÖ Cell 2: Load dataset\")\n",
    "    print(\"3. ‚úÖ Cell 3: Configure RAG settings\")\n",
    "    print(\"4. ‚úÖ Cell 4: Load/create chunks\")\n",
    "    print(\"5. ‚úÖ Cell 5: Initialize retriever\")\n",
    "    print(\"6. ‚úÖ Cell 6: Login to Hugging Face\")\n",
    "    print(\"7. ‚úÖ Cell 7: Load Mistral-7B model\")\n",
    "    print(\"8. ‚úÖ Cell 8: Initialize RAG generator\")\n",
    "    print(\"\\nThen run this cell again!\")\n",
    "    print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
