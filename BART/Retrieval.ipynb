{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc608ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 171,284 characters\n",
      "Estimated pages: ~85\n"
     ]
    }
   ],
   "source": [
    "with open('Dataset/StudentHandbookDataset.txt', 'r', encoding='utf-8') as f:\n",
    "    dataset = f.read()\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset):,} characters\")\n",
    "print(f\"Estimated pages: ~{len(dataset) // 2000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92fc4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bed4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Chunking (RUN ONCE ONLY)\n",
    "\n",
    "This section handles text chunking and saves the results. You only need to run this **once** or when:\n",
    "- You update your dataset\n",
    "- You want to change chunking strategy (e.g., chunk size, overlap)\n",
    "- The saved chunks file is deleted\n",
    "\n",
    "**Note**: After running once, skip to Step 2 for embedding experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4552b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Chunks file found! You can skip to Step 2 (Embedding Experiments)\n",
      "[INFO] To reload chunks, run: chunks = load_chunks()\n"
     ]
    }
   ],
   "source": [
    "def check_chunks_exist():\n",
    "    \"\"\"Check if chunks have been saved\"\"\"\n",
    "    return os.path.exists(\"saved_chunks/chunks.pkl\")\n",
    "\n",
    "def save_chunks(chunks, filename=\"saved_chunks/chunks.pkl\"):\n",
    "    \"\"\"Save chunks to disk\"\"\"\n",
    "    os.makedirs(\"saved_chunks\", exist_ok=True)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    print(f\"[SUCCESS] Saved {len(chunks)} chunks to {filename}\")\n",
    "\n",
    "def load_chunks(filename=\"saved_chunks/chunks.pkl\"):\n",
    "    \"\"\"Load chunks from disk\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            chunks = pickle.load(f)\n",
    "        print(f\"[SUCCESS] Loaded {len(chunks)} chunks from {filename}\")\n",
    "        return chunks\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] Chunks file not found: {filename}\")\n",
    "        return None\n",
    "\n",
    "# Check if chunks already exist\n",
    "if check_chunks_exist():\n",
    "    print(\"[SUCCESS] Chunks file found! You can skip to Step 2 (Embedding Experiments)\")\n",
    "    print(\"[INFO] To reload chunks, run: chunks = load_chunks()\")\n",
    "else:\n",
    "    print(\"[INFO] No saved chunks found. Run the chunking cells below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2e125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model for semantic chunking...\n",
      "   Using CPU for stable performance\n",
      "Setting up semantic chunker...\n",
      "[SUCCESS] Chunker ready (using CPU)!\n",
      "Setting up semantic chunker...\n",
      "[SUCCESS] Chunker ready (using CPU)!\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# EXPERIMENT PARAMETER: CHUNKING STRATEGY\n",
    "# ========================================\n",
    "# Change these values to experiment with different chunking!\n",
    "\n",
    "CHUNKING_STRATEGY = \"semantic\"  # Options: \"semantic\", \"fixed\" (future)\n",
    "PERCENTILE_THRESHOLD = 80       # Try: 70, 75, 80, 85, 90 (higher = fewer, larger chunks)\n",
    "BUFFER_SIZE = 1                 # Sentences to merge around breakpoints\n",
    "\n",
    "# ========================================\n",
    "\n",
    "# ONLY RUN THIS IF CHUNKS DON'T EXIST OR YOU WANT TO RE-CHUNK\n",
    "print(f\"Loading embedding model for {CHUNKING_STRATEGY} chunking...\")\n",
    "print(\"   Using CPU for stable performance\")\n",
    "\n",
    "chunking_embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(f\"Setting up semantic chunker with PERCENTILE={PERCENTILE_THRESHOLD}...\")\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=chunking_embed_model,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=PERCENTILE_THRESHOLD,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    add_start_index=True\n",
    ")\n",
    "print(f\"[SUCCESS] Chunker ready (Percentile: {PERCENTILE_THRESHOLD}, Buffer: {BUFFER_SIZE})!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa1d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating semantic chunks from raw text...\n",
      "[SUCCESS] Created 246 semantic chunks\n",
      "\n",
      "Chunk Analysis:\n",
      "   Average size: 650 characters\n",
      "   Size range: 2 - 7102 characters\n",
      "   Total chunks: 246\n",
      "\n",
      "Sample chunks:\n",
      "   Chunk 1: ﻿Republic of the Philippines  Eulogio \"Amang\" Rodriguez Institute of Science and Technology Office of Student Affairs and Services   EARIST STUDENT HA...\n",
      "   Chunk 2: ii - HISTORY OF EARIST ..... 1 - MISSION STATEMENTS   - Vision ..... 3   - Mission ..... 3   - Goal ........\n",
      "   Chunk 3: 3   - Objectives ..... 3 - CURRICULAR OFFERINGS   - Main Campus     - College of Architecture and Fine Arts ..... 4     - College of Arts and Sciences...\n",
      "[SUCCESS] Saved 246 chunks to saved_chunks/chunks.pkl\n",
      "\n",
      "[SUCCESS] Chunking complete and saved! You won't need to run this again.\n",
      "[SUCCESS] Created 246 semantic chunks\n",
      "\n",
      "Chunk Analysis:\n",
      "   Average size: 650 characters\n",
      "   Size range: 2 - 7102 characters\n",
      "   Total chunks: 246\n",
      "\n",
      "Sample chunks:\n",
      "   Chunk 1: ﻿Republic of the Philippines  Eulogio \"Amang\" Rodriguez Institute of Science and Technology Office of Student Affairs and Services   EARIST STUDENT HA...\n",
      "   Chunk 2: ii - HISTORY OF EARIST ..... 1 - MISSION STATEMENTS   - Vision ..... 3   - Mission ..... 3   - Goal ........\n",
      "   Chunk 3: 3   - Objectives ..... 3 - CURRICULAR OFFERINGS   - Main Campus     - College of Architecture and Fine Arts ..... 4     - College of Arts and Sciences...\n",
      "[SUCCESS] Saved 246 chunks to saved_chunks/chunks.pkl\n",
      "\n",
      "[SUCCESS] Chunking complete and saved! You won't need to run this again.\n"
     ]
    }
   ],
   "source": [
    "# ONLY RUN THIS IF CHUNKS DON'T EXIST OR YOU WANT TO RE-CHUNK\n",
    "print(\"Creating semantic chunks from raw text...\")\n",
    "print(f\"Configuration: Percentile={PERCENTILE_THRESHOLD}, Buffer={BUFFER_SIZE}\")\n",
    "chunks = text_splitter.create_documents([dataset])\n",
    "print(f\"[SUCCESS] Created {len(chunks)} semantic chunks\")\n",
    "\n",
    "# Analyze chunk quality\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "print(f\"\\nChunk Analysis:\")\n",
    "print(f\"   Average size: {np.mean(chunk_sizes):.0f} characters\")\n",
    "print(f\"   Size range: {min(chunk_sizes)} - {max(chunk_sizes)} characters\")\n",
    "print(f\"   Total chunks: {len(chunks)}\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(\"\\nSample chunks:\")\n",
    "for i in range(min(3, len(chunks))):\n",
    "    chunk_preview = chunks[i].page_content[:150].replace('\\n', ' ')\n",
    "    print(f\"   Chunk {i+1}: {chunk_preview}...\")\n",
    "\n",
    "# SAVE THE CHUNKS with configuration in filename\n",
    "chunk_filename = f\"saved_chunks/chunks_p{PERCENTILE_THRESHOLD}_b{BUFFER_SIZE}.pkl\"\n",
    "save_chunks(chunks, filename=chunk_filename)\n",
    "print(f\"\\n[SUCCESS] Chunking complete and saved to {chunk_filename}!\")\n",
    "print(f\"[INFO] Configuration: Percentile={PERCENTILE_THRESHOLD}, Buffer={BUFFER_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede42c88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load Chunks & Experiment with Embeddings\n",
    "\n",
    "Start here after chunking is done! This section lets you experiment with different embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc4640e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Loaded 246 chunks from saved_chunks/chunks.pkl\n",
      "[SUCCESS] Ready to experiment with 246 chunks!\n"
     ]
    }
   ],
   "source": [
    "# Load saved chunks\n",
    "chunks = load_chunks()\n",
    "\n",
    "if chunks is None:\n",
    "    print(\"[ERROR] Please run Step 1 (Chunking) first!\")\n",
    "else:\n",
    "    print(f\"[SUCCESS] Ready to experiment with {len(chunks)} chunks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc42cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose Your Embedding Model\n",
    "\n",
    "Experiment with different embedding models here! Uncomment the one you want to try:\n",
    "\n",
    "**Options:**\n",
    "- `all-mpnet-base-v2`: Best quality, slower (768 dim)\n",
    "- `all-MiniLM-L6-v2`: Fast, good quality (384 dim) \n",
    "- `multi-qa-mpnet-base-dot-v1`: Optimized for Q&A\n",
    "- `paraphrase-multilingual-mpnet-base-v2`: Multi-language support\n",
    "- Or try any model from https://huggingface.co/sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09544e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing embedding model: sentence-transformers/all-mpnet-base-v2\n",
      "   Using CPU for stable performance\n",
      "[SUCCESS] Embedding model loaded: all-mpnet-base-v2\n",
      "Embedding dimension: 768\n",
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# EXPERIMENT PARAMETER: EMBEDDING MODEL\n",
    "# ========================================\n",
    "# Change MODEL_NAME to experiment with different embeddings!\n",
    "\n",
    "# Current options (uncomment one):\n",
    "MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"  # Best quality, 768 dim\n",
    "# MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Fast, 384 dim\n",
    "# MODEL_NAME = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"  # Q&A optimized\n",
    "# MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"  # Multilingual\n",
    "\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"EMBEDDING MODEL CONFIGURATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"   Using CPU for stable performance\")\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_kwargs={\n",
    "        'device': 'cpu',\n",
    "        'trust_remote_code': True\n",
    "    },\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Get embedding dimension\n",
    "test_embed = embedding_model.embed_query(\"test\")\n",
    "\n",
    "print(f\"[SUCCESS] Embedding model loaded!\")\n",
    "print(f\"   Model: {MODEL_NAME.split('/')[-1]}\")\n",
    "print(f\"   Embedding dimension: {len(test_embed)}\")\n",
    "print(f\"   Device: CPU\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f5e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for all chunks with current model...\n",
      "   Processed batch 1/8\n",
      "   Processed batch 2/8\n",
      "   Processed batch 3/8\n",
      "   Processed batch 4/8\n",
      "   Processed batch 5/8\n",
      "   Processed batch 6/8\n",
      "   Processed batch 7/8\n",
      "   Processed batch 8/8\n",
      "\n",
      "[SUCCESS] Generated 246 embeddings in 36.53s\n",
      "Average: 0.148s per chunk\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating embeddings for all chunks with current model...\")\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "# Process embeddings in batches to avoid memory issues\n",
    "batch_size = 32\n",
    "all_embeddings = []\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(chunk_texts), batch_size):\n",
    "    batch = chunk_texts[i:i+batch_size]\n",
    "    batch_embeddings = embedding_model.embed_documents(batch)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    print(f\"   Processed batch {i//batch_size + 1}/{(len(chunk_texts) + batch_size - 1)//batch_size}\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n[SUCCESS] Generated {len(all_embeddings)} embeddings in {elapsed_time:.2f}s\")\n",
    "print(f\"Average: {elapsed_time/len(all_embeddings):.3f}s per chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d5daa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building FAISS vector database...\n",
      "[SUCCESS] FAISS index ready: 246 vectors (768 dimensions)\n",
      "Model: all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "# Build FAISS vector store for fast similarity search\n",
    "print(\"\\nBuilding FAISS vector database...\")\n",
    "dimension = len(all_embeddings[0])\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner Product for cosine similarity\n",
    "\n",
    "# Normalize embeddings for proper cosine similarity\n",
    "embeddings_array = np.array(all_embeddings).astype('float32')\n",
    "faiss.normalize_L2(embeddings_array)\n",
    "index.add(embeddings_array)\n",
    "\n",
    "print(f\"[SUCCESS] FAISS index ready: {index.ntotal:,} vectors ({dimension} dimensions)\")\n",
    "print(f\"Model: {MODEL_NAME.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b1df0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Retrieval Testing\n",
    "\n",
    "Now test your retrieval system with the current embedding model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cacb9040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Retrieval functions ready!\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_chunks(query, top_k=5):\n",
    "    \"\"\"Find most relevant chunks for the query\"\"\"\n",
    "    print(f\"\\nSearching for: '{query}'\")\n",
    "    print(f\"Model: {MODEL_NAME.split('/')[-1]}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Embed the query\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    query_vector = np.array([query_embedding]).astype('float32')\n",
    "    faiss.normalize_L2(query_vector)\n",
    "\n",
    "    # Search FAISS index\n",
    "    scores, indices = index.search(query_vector, top_k)\n",
    "\n",
    "    # Return results with metadata\n",
    "    results = []\n",
    "    for idx, score in zip(indices[0], scores[0]):\n",
    "        if idx < len(chunks):  # Safety check\n",
    "            chunk = chunks[idx]\n",
    "            results.append({\n",
    "                'text': chunk.page_content,\n",
    "                'score': float(score),\n",
    "                'chunk_id': int(idx),\n",
    "                'start_pos': chunk.metadata.get('start_index', 0) if hasattr(chunk, 'metadata') else 0\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "def display_retrieval_results(query, results):\n",
    "    \"\"\"Display comprehensive retrieval results\"\"\"\n",
    "    print(f\"Found {len(results)} relevant chunks for: '{query}'\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nChunk {i} (ID: {result['chunk_id']})\")\n",
    "        print(f\"Relevance Score: {result['score']:.4f}\")\n",
    "        print(f\"Position in Document: Character {result['start_pos']:,}\")\n",
    "        print(f\"Length: {len(result['text'])} characters\")\n",
    "        print(f\"Content Preview (first 200 chars):\")\n",
    "        print(f\"   {result['text'][:200].replace(chr(10), ' ').replace(chr(13), '')}...\")\n",
    "        \n",
    "        # Show full content if it's short enough\n",
    "        if len(result['text']) <= 500:\n",
    "            print(f\"Full Content:\")\n",
    "            print(f\"   {result['text']}\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"[SUCCESS] Retrieval functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd4f763",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Comprehensive Testing with Questions.json\n",
    "\n",
    "Test the retrieval system against a comprehensive set of questions organized by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4102ab42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Loaded 50 questions across 8 categories\n",
      "\n",
      "Categories:\n",
      "   • Admissions: 8 questions\n",
      "   • Enrollment and Registration: 8 questions\n",
      "   • Fees, Payments and Scholarships: 6 questions\n",
      "   • Academic Policies and Grading: 8 questions\n",
      "   • Attendance and Conduct: 6 questions\n",
      "   • Student Services and Organizations: 6 questions\n",
      "   • Graduation and Academic Completion: 5 questions\n",
      "   • General and Miscellaneous Information: 3 questions\n"
     ]
    }
   ],
   "source": [
    "# Load questions from Questions.json\n",
    "import json\n",
    "\n",
    "with open('Questions.json', 'r', encoding='utf-8') as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "# Count total questions\n",
    "total_questions = sum(len(questions) for questions in questions_data.values())\n",
    "print(f\"[SUCCESS] Loaded {total_questions} questions across {len(questions_data)} categories\")\n",
    "print(\"\\nCategories:\")\n",
    "for category, questions in questions_data.items():\n",
    "    print(f\"   • {category}: {len(questions)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73ddbf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Category testing function ready!\n"
     ]
    }
   ],
   "source": [
    "def test_category_questions(category_name, top_k=3, show_full_results=False):\n",
    "    \"\"\"Test all questions from a specific category\"\"\"\n",
    "    if category_name not in questions_data:\n",
    "        print(f\"[ERROR] Category '{category_name}' not found!\")\n",
    "        print(f\"Available categories: {', '.join(questions_data.keys())}\")\n",
    "        return\n",
    "    \n",
    "    questions = questions_data[category_name]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing Category: {category_name}\")\n",
    "    print(f\"Total Questions: {len(questions)}\")\n",
    "    print(f\"Model: {MODEL_NAME.split('/')[-1]}\")\n",
    "    print(f\"Retrieving top {top_k} chunks per question\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, q_item in enumerate(questions, 1):\n",
    "        question = q_item['question']\n",
    "        expected_ref = q_item['expected_reference']\n",
    "        \n",
    "        print(f\"\\n{'─'*70}\")\n",
    "        print(f\"Question {i}/{len(questions)}\")\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"Expected Reference: {expected_ref}\")\n",
    "        print(f\"{'─'*70}\")\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        results = retrieve_relevant_chunks(question, top_k=top_k)\n",
    "        \n",
    "        # Store results for analysis\n",
    "        all_results.append({\n",
    "            'question': question,\n",
    "            'expected_reference': expected_ref,\n",
    "            'results': results,\n",
    "            'top_score': results[0]['score'] if results else 0\n",
    "        })\n",
    "        \n",
    "        if show_full_results:\n",
    "            display_retrieval_results(question, results)\n",
    "        else:\n",
    "            # Show compact summary\n",
    "            print(f\"\\nTop {min(top_k, len(results))} Results:\")\n",
    "            for j, result in enumerate(results[:top_k], 1):\n",
    "                preview = result['text'][:150].replace('\\n', ' ').replace('\\r', '')\n",
    "                print(f\"   {j}. Score: {result['score']:.4f} | {preview}...\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CATEGORY SUMMARY: {category_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    avg_top_score = np.mean([r['top_score'] for r in all_results])\n",
    "    print(f\"Average Top Score: {avg_top_score:.4f}\")\n",
    "    print(f\"Score Range: {min([r['top_score'] for r in all_results]):.4f} - {max([r['top_score'] for r in all_results]):.4f}\")\n",
    "    \n",
    "    # Score distribution\n",
    "    high_confidence = sum(1 for r in all_results if r['top_score'] > 0.7)\n",
    "    medium_confidence = sum(1 for r in all_results if 0.5 <= r['top_score'] <= 0.7)\n",
    "    low_confidence = sum(1 for r in all_results if r['top_score'] < 0.5)\n",
    "    \n",
    "    print(f\"\\nConfidence Distribution:\")\n",
    "    print(f\"   High (>0.7):   {high_confidence}/{len(all_results)} questions ({high_confidence/len(all_results)*100:.1f}%)\")\n",
    "    print(f\"   Medium (0.5-0.7): {medium_confidence}/{len(all_results)} questions ({medium_confidence/len(all_results)*100:.1f}%)\")\n",
    "    print(f\"   Low (<0.5):    {low_confidence}/{len(all_results)} questions ({low_confidence/len(all_results)*100:.1f}%)\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"[SUCCESS] Category testing function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bdfd590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Comprehensive testing function ready!\n"
     ]
    }
   ],
   "source": [
    "def test_all_questions(top_k=3, show_detailed=False):\n",
    "    \"\"\"Test all questions from all categories\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"COMPREHENSIVE RETRIEVAL TEST\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Model: {MODEL_NAME.split('/')[-1]}\")\n",
    "    print(f\"Total Categories: {len(questions_data)}\")\n",
    "    print(f\"Total Questions: {sum(len(q) for q in questions_data.values())}\")\n",
    "    print(f\"Top-K per question: {top_k}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    all_category_results = {}\n",
    "    \n",
    "    for category_name in questions_data.keys():\n",
    "        print(f\"\\n\\n{'█'*70}\")\n",
    "        print(f\"CATEGORY: {category_name}\")\n",
    "        print(f\"{'█'*70}\")\n",
    "        \n",
    "        category_results = test_category_questions(\n",
    "            category_name, \n",
    "            top_k=top_k, \n",
    "            show_full_results=show_detailed\n",
    "        )\n",
    "        all_category_results[category_name] = category_results\n",
    "    \n",
    "    # Overall summary\n",
    "    print(f\"\\n\\n{'='*70}\")\n",
    "    print(f\"OVERALL PERFORMANCE SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    all_scores = []\n",
    "    for category, results in all_category_results.items():\n",
    "        scores = [r['top_score'] for r in results]\n",
    "        all_scores.extend(scores)\n",
    "        avg = np.mean(scores)\n",
    "        print(f\"\\n{category}:\")\n",
    "        print(f\"   Questions: {len(results)}\")\n",
    "        print(f\"   Avg Score: {avg:.4f}\")\n",
    "        print(f\"   Range: {min(scores):.4f} - {max(scores):.4f}\")\n",
    "    \n",
    "    # Global statistics\n",
    "    print(f\"\\n{'─'*70}\")\n",
    "    print(f\"GLOBAL STATISTICS:\")\n",
    "    print(f\"   Total Questions Tested: {len(all_scores)}\")\n",
    "    print(f\"   Overall Average Score: {np.mean(all_scores):.4f}\")\n",
    "    print(f\"   Median Score: {np.median(all_scores):.4f}\")\n",
    "    print(f\"   Std Deviation: {np.std(all_scores):.4f}\")\n",
    "    \n",
    "    # Overall confidence distribution\n",
    "    high = sum(1 for s in all_scores if s > 0.7)\n",
    "    medium = sum(1 for s in all_scores if 0.5 <= s <= 0.7)\n",
    "    low = sum(1 for s in all_scores if s < 0.5)\n",
    "    \n",
    "    print(f\"\\n   Overall Confidence Distribution:\")\n",
    "    print(f\"      High (>0.7):   {high}/{len(all_scores)} ({high/len(all_scores)*100:.1f}%)\")\n",
    "    print(f\"      Medium (0.5-0.7): {medium}/{len(all_scores)} ({medium/len(all_scores)*100:.1f}%)\")\n",
    "    print(f\"      Low (<0.5):    {low}/{len(all_scores)} ({low/len(all_scores)*100:.1f}%)\")\n",
    "    \n",
    "    return all_category_results\n",
    "\n",
    "print(\"[SUCCESS] Comprehensive testing function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1304401",
   "metadata": {},
   "source": [
    "### Testing Individual Categories (Optional)\n",
    "\n",
    "If you want to test a specific category before running full experiments:\n",
    "\n",
    "```python\n",
    "test_category_questions(\"Admissions\", top_k=5, show_full_results=False)\n",
    "```\n",
    "\n",
    "**Available Categories:**\n",
    "- Admissions\n",
    "- Enrollment and Registration  \n",
    "- Fees, Payments and Scholarships\n",
    "- Academic Policies and Grading\n",
    "- Attendance and Conduct\n",
    "- Student Services and Organizations\n",
    "- Graduation and Academic Completion\n",
    "- General and Miscellaneous Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d165ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Experiment Tracking & Comparison\n",
    "\n",
    "Track and compare different chunking strategies and embedding models to find the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1af06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment tracking system\n",
    "experiment_results = []\n",
    "\n",
    "def save_experiment_results(filename=\"experiment_results.json\"):\n",
    "    \"\"\"Save all experiment results to file\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(experiment_results, f, indent=2)\n",
    "    print(f\"[SUCCESS] Saved {len(experiment_results)} experiments to {filename}\")\n",
    "\n",
    "def load_experiment_results(filename=\"experiment_results.json\"):\n",
    "    \"\"\"Load previous experiment results\"\"\"\n",
    "    global experiment_results\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            experiment_results = json.load(f)\n",
    "        print(f\"[SUCCESS] Loaded {len(experiment_results)} previous experiments\")\n",
    "        return experiment_results\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[INFO] No previous results found. Starting fresh!\")\n",
    "        return []\n",
    "\n",
    "# Try to load previous results\n",
    "load_experiment_results()\n",
    "\n",
    "print(\"[SUCCESS] Experiment tracking system ready!\")\n",
    "print(f\"Current experiments tracked: {len(experiment_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_experiment(experiment_name, chunking_config, embedding_model_name, top_k=3):\n",
    "    \"\"\"\n",
    "    Run a complete experiment with specific chunking and embedding configuration\n",
    "    \n",
    "    Args:\n",
    "        experiment_name: Descriptive name for this experiment\n",
    "        chunking_config: Dict with keys 'strategy', 'params' (e.g., percentile, overlap)\n",
    "        embedding_model_name: HuggingFace model name\n",
    "        top_k: Number of chunks to retrieve per query\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'█'*70}\")\n",
    "    print(f\"RUNNING EXPERIMENT: {experiment_name}\")\n",
    "    print(f\"{'█'*70}\")\n",
    "    print(f\"Chunking: {chunking_config}\")\n",
    "    print(f\"Embedding: {embedding_model_name}\")\n",
    "    print(f\"Top-K: {top_k}\")\n",
    "    print(f\"{'█'*70}\\n\")\n",
    "    \n",
    "    # Test with all questions\n",
    "    category_results = test_all_questions(top_k=top_k, show_detailed=False)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_scores = []\n",
    "    category_scores = {}\n",
    "    \n",
    "    for category, results in category_results.items():\n",
    "        scores = [r['top_score'] for r in results]\n",
    "        all_scores.extend(scores)\n",
    "        category_scores[category] = {\n",
    "            'avg_score': float(np.mean(scores)),\n",
    "            'min_score': float(min(scores)),\n",
    "            'max_score': float(max(scores)),\n",
    "            'num_questions': len(scores)\n",
    "        }\n",
    "    \n",
    "    # Compute overall metrics\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    experiment_data = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'chunking_config': chunking_config,\n",
    "        'embedding_model': embedding_model_name,\n",
    "        'top_k': top_k,\n",
    "        'metrics': {\n",
    "            'overall_avg_score': float(np.mean(all_scores)),\n",
    "            'overall_median_score': float(np.median(all_scores)),\n",
    "            'overall_std': float(np.std(all_scores)),\n",
    "            'min_score': float(min(all_scores)),\n",
    "            'max_score': float(max(all_scores)),\n",
    "            'high_confidence_pct': float(sum(1 for s in all_scores if s > 0.7) / len(all_scores) * 100),\n",
    "            'medium_confidence_pct': float(sum(1 for s in all_scores if 0.5 <= s <= 0.7) / len(all_scores) * 100),\n",
    "            'low_confidence_pct': float(sum(1 for s in all_scores if s < 0.5) / len(all_scores) * 100),\n",
    "            'total_questions': len(all_scores),\n",
    "            'execution_time_seconds': elapsed_time\n",
    "        },\n",
    "        'category_metrics': category_scores\n",
    "    }\n",
    "    \n",
    "    # Save to tracking\n",
    "    experiment_results.append(experiment_data)\n",
    "    save_experiment_results()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT COMPLETE: {experiment_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Overall Avg Score: {experiment_data['metrics']['overall_avg_score']:.4f}\")\n",
    "    print(f\"High Confidence: {experiment_data['metrics']['high_confidence_pct']:.1f}%\")\n",
    "    print(f\"Execution Time: {elapsed_time:.1f}s\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return experiment_data\n",
    "\n",
    "print(\"[SUCCESS] Experiment runner ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe10d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_experiments():\n",
    "    \"\"\"Compare all tracked experiments and show rankings\"\"\"\n",
    "    if not experiment_results:\n",
    "        print(\"[INFO] No experiments to compare yet. Run some experiments first!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENT COMPARISON DASHBOARD\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total Experiments: {len(experiment_results)}\\n\")\n",
    "    \n",
    "    # Sort by overall average score\n",
    "    sorted_experiments = sorted(\n",
    "        experiment_results, \n",
    "        key=lambda x: x['metrics']['overall_avg_score'], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Display ranking table\n",
    "    print(f\"{'Rank':<6} {'Experiment Name':<35} {'Avg Score':<12} {'High Conf %':<12} {'Time (s)':<10}\")\n",
    "    print(f\"{'─'*80}\")\n",
    "    \n",
    "    for i, exp in enumerate(sorted_experiments, 1):\n",
    "        name = exp['experiment_name'][:33]\n",
    "        avg_score = exp['metrics']['overall_avg_score']\n",
    "        high_conf = exp['metrics']['high_confidence_pct']\n",
    "        exec_time = exp['metrics']['execution_time_seconds']\n",
    "        \n",
    "        print(f\"{i:<6} {name:<35} {avg_score:<12.4f} {high_conf:<12.1f} {exec_time:<10.1f}\")\n",
    "    \n",
    "    # Show best experiment details\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🏆 BEST EXPERIMENT: {sorted_experiments[0]['experiment_name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    best = sorted_experiments[0]\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"   Chunking: {best['chunking_config']}\")\n",
    "    print(f\"   Embedding Model: {best['embedding_model']}\")\n",
    "    print(f\"   Top-K: {best['top_k']}\")\n",
    "    \n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"   Average Score: {best['metrics']['overall_avg_score']:.4f}\")\n",
    "    print(f\"   Median Score: {best['metrics']['overall_median_score']:.4f}\")\n",
    "    print(f\"   Std Deviation: {best['metrics']['overall_std']:.4f}\")\n",
    "    print(f\"   Score Range: {best['metrics']['min_score']:.4f} - {best['metrics']['max_score']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfidence Distribution:\")\n",
    "    print(f\"   High (>0.7):   {best['metrics']['high_confidence_pct']:.1f}%\")\n",
    "    print(f\"   Medium (0.5-0.7): {best['metrics']['medium_confidence_pct']:.1f}%\")\n",
    "    print(f\"   Low (<0.5):    {best['metrics']['low_confidence_pct']:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nCategory Performance:\")\n",
    "    for category, metrics in best['category_metrics'].items():\n",
    "        print(f\"   {category}: {metrics['avg_score']:.4f}\")\n",
    "    \n",
    "    # Show comparison with worst\n",
    "    if len(sorted_experiments) > 1:\n",
    "        worst = sorted_experiments[-1]\n",
    "        improvement = ((best['metrics']['overall_avg_score'] - worst['metrics']['overall_avg_score']) \n",
    "                      / worst['metrics']['overall_avg_score'] * 100)\n",
    "        print(f\"\\n📈 Improvement over worst: {improvement:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    return sorted_experiments\n",
    "\n",
    "print(\"[SUCCESS] Comparison function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbb47b",
   "metadata": {},
   "source": [
    "### 🎯 Experiment Workflow\n",
    "\n",
    "**To run an experiment:**\n",
    "\n",
    "1. **Set parameters** in Step 1 (chunking) and Step 2 (embedding)\n",
    "2. **Run those cells** to generate chunks and embeddings\n",
    "3. **Run the experiment cell below**\n",
    "4. **View results** with the comparison cells\n",
    "\n",
    "The experiment will automatically test all 60+ questions and save results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e4573",
   "metadata": {},
   "source": [
    "### 📋 Suggested Testing Plan\n",
    "\n",
    "**Phase 1: Find Best Embedding Model** (keep chunking at P=80)\n",
    "- Test: MiniLM, MPNet, multi-qa-mpnet\n",
    "- Takes ~15 minutes\n",
    "\n",
    "**Phase 2: Find Best Chunking** (use best model from Phase 1)\n",
    "- Test percentiles: 70, 75, 80, 85, 90\n",
    "- Takes ~25 minutes\n",
    "\n",
    "**Phase 3: Optimize Top-K** (use best combo)\n",
    "- Test: top_k = 3, 5, 7\n",
    "- Takes ~15 minutes\n",
    "\n",
    "**Total: ~55 minutes for complete optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341d96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization of experiment results\n",
    "def plot_experiment_comparison():\n",
    "    \"\"\"Create a simple text-based comparison chart\"\"\"\n",
    "    if not experiment_results:\n",
    "        print(\"[INFO] No experiments to visualize yet.\")\n",
    "        return\n",
    "    \n",
    "    sorted_exp = sorted(experiment_results, key=lambda x: x['metrics']['overall_avg_score'], reverse=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPERIMENT PERFORMANCE VISUALIZATION\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Bar chart of average scores\n",
    "    print(\"Average Score Comparison:\")\n",
    "    print(\"-\" * 70)\n",
    "    max_score = max(e['metrics']['overall_avg_score'] for e in sorted_exp)\n",
    "    \n",
    "    for exp in sorted_exp:\n",
    "        name = exp['experiment_name'][:30]\n",
    "        score = exp['metrics']['overall_avg_score']\n",
    "        bar_length = int((score / max_score) * 40)\n",
    "        bar = '█' * bar_length\n",
    "        print(f\"{name:<30} {score:.4f} {bar}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # High confidence percentage comparison\n",
    "    print(\"High Confidence (>0.7) Percentage:\")\n",
    "    print(\"-\" * 70)\n",
    "    max_conf = max(e['metrics']['high_confidence_pct'] for e in sorted_exp)\n",
    "    \n",
    "    for exp in sorted_exp:\n",
    "        name = exp['experiment_name'][:30]\n",
    "        conf = exp['metrics']['high_confidence_pct']\n",
    "        bar_length = int((conf / max_conf) * 40) if max_conf > 0 else 0\n",
    "        bar = '█' * bar_length\n",
    "        print(f\"{name:<30} {conf:>6.1f}% {bar}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "print(\"[SUCCESS] Visualization function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439749b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Start Guide for Experimentation\n",
    "\n",
    "### **Step-by-Step Workflow:**\n",
    "\n",
    "1. **Set Chunking Parameters** (Go to Step 1, Cell 5)\n",
    "   - Change `PERCENTILE_THRESHOLD` (70-90)\n",
    "   - Run chunking cells\n",
    "\n",
    "2. **Set Embedding Model** (Go to Step 2, Cell 10)\n",
    "   - Uncomment desired `MODEL_NAME`\n",
    "   - Run embedding cells\n",
    "\n",
    "3. **Run Experiment** (Below)\n",
    "   - Execute the experiment cell\n",
    "   - Results auto-saved\n",
    "\n",
    "4. **Compare Results** (Below)\n",
    "   - View rankings and metrics\n",
    "   - See visualizations\n",
    "\n",
    "5. **Repeat** with different configs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3ec8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# RUN CURRENT EXPERIMENT\n",
    "# ========================================\n",
    "# This will test your current chunking + embedding configuration\n",
    "# against all questions from Questions.json\n",
    "\n",
    "# Automatically detect current configuration\n",
    "current_chunking = {\n",
    "    \"strategy\": \"semantic\",\n",
    "    \"percentile\": PERCENTILE_THRESHOLD if 'PERCENTILE_THRESHOLD' in dir() else 80,\n",
    "    \"buffer_size\": BUFFER_SIZE if 'BUFFER_SIZE' in dir() else 1\n",
    "}\n",
    "\n",
    "current_model = MODEL_NAME.split('/')[-1] if 'MODEL_NAME' in dir() else \"unknown\"\n",
    "\n",
    "# Generate experiment name\n",
    "experiment_name = f\"P{current_chunking['percentile']}-{current_model}\"\n",
    "\n",
    "print(f\"Ready to run experiment: {experiment_name}\")\n",
    "print(f\"Chunking: {current_chunking}\")\n",
    "print(f\"Embedding: {MODEL_NAME if 'MODEL_NAME' in dir() else 'Not set'}\")\n",
    "print(\"\\nUncomment the line below to run:\")\n",
    "print(\"# run_full_experiment(experiment_name, current_chunking, MODEL_NAME, top_k=5)\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# run_full_experiment(experiment_name, current_chunking, MODEL_NAME, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# VIEW RESULTS & COMPARISON\n",
    "# ========================================\n",
    "\n",
    "# Show comparison dashboard\n",
    "compare_experiments()\n",
    "\n",
    "# Show visual comparison\n",
    "plot_experiment_comparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
